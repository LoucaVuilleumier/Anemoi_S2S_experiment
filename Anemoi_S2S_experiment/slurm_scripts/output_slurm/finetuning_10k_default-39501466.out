[ECMWF-INFO -sbatch] - -------------------------------------------------------------------------------------
[ECMWF-INFO -sbatch] -  This is the ECMWF jobfilter
[ECMWF-INFO -sbatch] -  +++ Please report issues using the Support portal +++
[ECMWF-INFO -sbatch] -  +++ https://support.ecmwf.int                     +++
[ECMWF-INFO -sbatch] -  /usr/local/bin/ecsbatch: size: 53801, mtime: Tue Nov 25 12:31:30 2025
[ECMWF-INFO -sbatch] - -------------------------------------------------------------------------------------
[ECMWF-INFO -sbatch] - Time at submit: Mon Dec 15 15:00:09 2025 (1765810809.001633) on ac6-100.bullx:/lus/h2resw01/hpcperm/nld4584/Anemoi_S2S_experiment/slurm_scripts
[ECMWF-INFO -sbatch] - --- SLURM VARIABLES ---
[ECMWF-INFO -sbatch] - EC_CLUSTER=ac
[ECMWF-INFO -sbatch] - SLURM_EXPORT_ENV=ALL
[ECMWF-INFO -sbatch] - SBATCH_EXPORT=NONE
[ECMWF-INFO -sbatch] - -----------------------
[ECMWF-INFO -sbatch] - jobscript: /lus/h2resw01/hpcperm/nld4584/Anemoi_S2S_experiment/slurm_scripts/finetuning_10k_default.sh
[ECMWF-INFO -sbatch] - --- SCRIPT OPTIONS ---
[ECMWF-INFO -sbatch] - #SBATCH --job-name=finetuning_10k_default
[ECMWF-INFO -sbatch] - #SBATCH --output=./slurm_scripts/output_slurm/finetuning_10k_default-%J.out
[ECMWF-INFO -sbatch] - #SBATCH --error=./slurm_scripts/output_slurm/finetuning_10k_default-%J.out
[ECMWF-INFO -sbatch] - #SBATCH --chdir=/ec/res4/hpcperm/nld4584/Anemoi_S2S_experiment
[ECMWF-INFO -sbatch] - #SBATCH --qos=ng
[ECMWF-INFO -sbatch] - #SBATCH --partition=gpu
[ECMWF-INFO -sbatch] - #SBATCH --nodes=4
[ECMWF-INFO -sbatch] - #SBATCH --ntasks-per-node=4
[ECMWF-INFO -sbatch] - #SBATCH --cpus-per-task=32
[ECMWF-INFO -sbatch] - #SBATCH --gpus-per-node=4
[ECMWF-INFO -sbatch] - #SBATCH --mem=460G
[ECMWF-INFO -sbatch] - #SBATCH --time=24:00:00
[ECMWF-INFO -sbatch] - -----------------------
[ECMWF-INFO -sbatch] - --- POST-PROCESSED OPTIONS ---
[ECMWF-INFO -sbatch] - ARG --positional=['./finetuning_10k_default.sh']
[ECMWF-INFO -sbatch] - ARG --cpus_per_task=32
[ECMWF-INFO -sbatch] - ARG --chdir=/ec/res4/hpcperm/nld4584/Anemoi_S2S_experiment
[ECMWF-INFO -sbatch] - ARG --error=./slurm_scripts/output_slurm/finetuning_10k_default-%J.out
[ECMWF-INFO -sbatch] - ARG --job_name=finetuning_10k_default
[ECMWF-INFO -sbatch] - ARG --ntasks_per_node=4
[ECMWF-INFO -sbatch] - ARG --nodes=4
[ECMWF-INFO -sbatch] - ARG --output=./slurm_scripts/output_slurm/finetuning_10k_default-%J.out
[ECMWF-INFO -sbatch] - ARG --partition=gpu
[ECMWF-INFO -sbatch] - ARG --qos=ng
[ECMWF-INFO -sbatch] - ARG --time=24:00:00
[ECMWF-INFO -sbatch] - ARG --mem=460G
[ECMWF-INFO -sbatch] - ARG --gpus_per_node=4
[ECMWF-INFO -sbatch] - ------------------------------
[ECMWF-INFO -sbatch] - jobtag: nld4584-finetuning_0k_default-4x128-/lus/h2resw01/hpcperm/nld4584/Anemoi_S2S_experiment/slurm_scripts/./slurm_scripts/output_slurm/finetuning_10k_default-_.out
[ECMWF-INFO -sbatch] - ['/usr/bin/sbatch', '--cpus-per-task=32', '--chdir=/ec/res4/hpcperm/nld4584/Anemoi_S2S_experiment', '--error=./slurm_scripts/output_slurm/finetuning_10k_default-%J.out', '--job-name=finetuning_10k_default', '--ntasks-per-node=4', '--nodes=4', '--output=./slurm_scripts/output_slurm/finetuning_10k_default-%J.out', '--partition=gpu', '--qos=ng', '--time=24:00:00', '--mem=460G', '--gpus-per-node=4', '--licenses=h2resw01', '--export=EC_user_time_limit=24:00:00', '/lus/h2resw01/hpcperm/nld4584/Anemoi_S2S_experiment/slurm_scripts/finetuning_10k_default.sh']
[ECMWF-INFO -sbatch] - sbatch executed on ac
[ECMWF-INFO -sbatch] - Job queued on ac using method local
[ECMWF-INFO -sbatch] - Submitted batch job 39501466
[ECMWF-INFO -ecprofile] /usr/bin/bash NON_INTERACTIVE on ac6-306 at 20251215_192224.591, PID: 26561, JOBID: 39501466
[ECMWF-INFO -ecprofile] $SCRATCH=/ec/res4/scratch/nld4584
[ECMWF-INFO -ecprofile] $PERM=/perm/nld4584
[ECMWF-INFO -ecprofile] $HPCPERM=/ec/res4/hpcperm/nld4584
[ECMWF-INFO -ecprofile] $TMPDIR=/dev/shm/_tmpdir_.nld4584.39501466
[ECMWF-INFO -ecprofile] $SCRATCHDIR=/ec/res4/scratchdir/nld4584/2/39501466
2025-12-15 19:29:07 INFO Running anemoi training command with overrides: ['--config-name=finetuning_10k_default.yaml']
2025-12-15 19:30:25 INFO Prepending Anemoi Config Env (/ec/res4/hpcperm/nld4584/Anemoi_S2S_experiment/Configs_new/Training) to the search path.
2025-12-15 19:30:25 INFO Prepending current user directory (/lus/h2resw01/hpcperm/nld4584/Anemoi_S2S_experiment) to the search path.
2025-12-15 19:30:25 INFO Search path is now: [provider=anemoi-cwd-searchpath-plugin, path=/lus/h2resw01/hpcperm/nld4584/Anemoi_S2S_experiment, provider=anemoi-env-searchpath-plugin, path=/ec/res4/hpcperm/nld4584/Anemoi_S2S_experiment/Configs_new/Training, provider=hydra, path=pkg://hydra.conf, provider=main, path=pkg://anemoi.training/config]
[2025-12-15 19:30:27,035][anemoi.training.train.train][INFO] - Skipping config validation.
[2025-12-15 19:30:27,055][anemoi.training.train.train][INFO] - Starting from checkpoint: True
[2025-12-15 19:30:27,057][anemoi.training.diagnostics.logger][INFO] - Maximum number of params allowed to be logged is: 2000
[2025-12-15 19:30:27,060][anemoi.utils.config][INFO] - Using environment variable ANEMOI_CONFIG_PATH to override the anemoi config key 'path.'
/lus/h2resw01/hpcperm/nld4584/anemoi_core_2025_17_11/anemoi-utils/src/anemoi/utils/config.py:207: UserWarning: Mofifying and instance of DotDict(). This class is intended to be immutable.
  warnings.warn("Mofifying and instance of DotDict(). This class is intended to be immutable.")
[2025-12-15 19:30:27,094][anemoi.training.diagnostics.mlflow.logger][INFO] - MLflow is logging offline.
Experiment with name Louca_finetuning not found. Creating it.
[2025-12-15 19:30:57,612][anemoi.training.diagnostics.mlflow.logger][INFO] - Logging 1 parameters
[2025-12-15 19:30:57,621][anemoi.training.diagnostics.mlflow.logger][INFO] - Terminal Log Path: output_training/plots/9751245a1372402e9e7b9bfd814b099a/plots/terminal_log.txt
[2025-12-15 19:30:57,622][anemoi.training.diagnostics.mlflow.logger][INFO] - SLURM job id: 39501466
[2025-12-15 19:30:57,622][anemoi.training.train.train][INFO] - Mlflow Run id: 9751245a1372402e9e7b9bfd814b099a
[2025-12-15 19:30:57,622][anemoi.training.train.train][INFO] - Run id: 9751245a1372402e9e7b9bfd814b099a
[2025-12-15 19:30:57,622][anemoi.training.train.train][INFO] - Parent run server2server: None
[2025-12-15 19:30:57,622][anemoi.training.train.train][INFO] - Fork run server2server: None
[2025-12-15 19:30:57,622][anemoi.training.train.train][INFO] - Checkpoints path: output_training/checkpoint/9751245a1372402e9e7b9bfd814b099a
[2025-12-15 19:30:57,622][anemoi.training.train.train][INFO] - Plots path: output_training/plots/9751245a1372402e9e7b9bfd814b099a
[2025-12-15 19:30:57,622][anemoi.training.train.train][INFO] - Dry run: False
/lus/h2resw01/hpcperm/nld4584/anemoi_core_2025_17_11/anemoi-utils/src/anemoi/utils/config.py:207: UserWarning: Mofifying and instance of DotDict(). This class is intended to be immutable.
  warnings.warn("Mofifying and instance of DotDict(). This class is intended to be immutable.")
[2025-12-15 19:31:44,083][anemoi.graphs.nodes.builders.from_file][INFO] - Reading the dataset from /home/mlx/ai-ml/datasets//aifs-ea-an-oper-0001-mars-o96-1979-2023-6h-v8.zarr.
[2025-12-15 19:31:49,919][anemoi.utils.config][INFO] - Using environment variable ANEMOI_CONFIG_PATH to override the anemoi config key 'path.'
[2025-12-15 19:31:58,870][anemoi.graphs.edges.builders.cutoff][INFO] - Using CutOff-Edges (with radius = 144.6 km) between data and hidden.
[2025-12-15 19:31:59,651][anemoi.graphs.edges.builders.base][WARNING] - The 'torch-cluster' library is not installed. Installing 'torch-cluster' can significantly improve performance for graph creation. You can install it using 'pip install torch-cluster'.
[2025-12-15 19:32:05,645][anemoi.graphs.edges.builders.knn][INFO] - Using KNN-Edges (with 3 nearest neighbours) between hidden and data.
[2025-12-15 19:32:05,646][anemoi.graphs.edges.builders.base][WARNING] - The 'torch-cluster' library is not installed. Installing 'torch-cluster' can significantly improve performance for graph creation. You can install it using 'pip install torch-cluster'.
[2025-12-15 19:32:05,721][anemoi.graphs.create][INFO] - Cleaning graph.
[2025-12-15 19:32:05,722][anemoi.graphs.create][INFO] - _grid_reference_distance deleted from graph.
[2025-12-15 19:32:05,722][anemoi.graphs.create][INFO] - _dataset deleted from graph.
[2025-12-15 19:32:05,722][anemoi.graphs.create][INFO] - _grid_reference_distance deleted from graph.
[2025-12-15 19:32:06,033][anemoi.graphs.create][INFO] - Graph saved at graphs/graph.pkl.
[2025-12-15 19:32:07,426][anemoi.training.data.datamodule.singledatamodule][WARNING] - Falling back rollout to: 1
[2025-12-15 19:32:07,426][anemoi.training.data.datamodule.singledatamodule][INFO] - Timeincrement set to 1 for data with frequency, 21600, and timestep, 21600
[2025-12-15 19:32:07,429][anemoi.training.train.train][INFO] - Number of data variables: 101
[2025-12-15 19:33:02,270][anemoi.training.diagnostics.callbacks.plot][INFO] - Setting up asynchronous plotting ...
[2025-12-15 19:33:02,272][anemoi.training.diagnostics.callbacks.plot][INFO] - Setting up asynchronous plotting ...
[2025-12-15 19:33:02,319][anemoi.training.diagnostics.callbacks.plot][INFO] - Setting up asynchronous plotting ...
[2025-12-15 19:33:02,320][anemoi.training.diagnostics.callbacks.plot][INFO] - Using defined accumulation colormap for fields: ['tp', 'cp']
[2025-12-15 19:33:02,322][anemoi.training.diagnostics.callbacks.plot][INFO] - Setting up asynchronous plotting ...
[2025-12-15 19:33:02,323][anemoi.training.diagnostics.callbacks.plot][INFO] - Setting up asynchronous plotting ...
[2025-12-15 19:33:02,324][anemoi.training.diagnostics.callbacks.plot][INFO] - Using precip histogram plotting method for fields: ['tp', 'cp'].
[2025-12-15 19:33:02,466][anemoi.training.train.train][INFO] - MLFlow logger enabled
Using 16bit Automatic Mixed Precision (AMP)
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
[rank: 0] Seed set to 42000
/lus/h2resw01/hpcperm/nld4584/anemoi_core_2025_17_11/anemoi-utils/src/anemoi/utils/provenance.py:143: UserWarning: The '__version__' attribute is deprecated and will be removed in MarkupSafe 3.1. Use feature detection, or `importlib.metadata.version("markupsafe")`, instead.
  versions[name] = str(module.__version__)
[2025-12-15 19:36:14,600][anemoi.models.preprocessing.normalizer][INFO] - Normalizing: cos_julian_day is not normalized.
[2025-12-15 19:36:14,600][anemoi.models.preprocessing.normalizer][INFO] - Normalizing: cos_latitude is not normalized.
[2025-12-15 19:36:14,600][anemoi.models.preprocessing.normalizer][INFO] - Normalizing: cos_local_time is not normalized.
[2025-12-15 19:36:14,600][anemoi.models.preprocessing.normalizer][INFO] - Normalizing: cos_longitude is not normalized.
[2025-12-15 19:36:14,600][anemoi.models.preprocessing.normalizer][INFO] - Normalizing: insolation is not normalized.
[2025-12-15 19:36:14,600][anemoi.models.preprocessing.normalizer][INFO] - Normalizing: lsm is not normalized.
[2025-12-15 19:36:14,601][anemoi.models.preprocessing.normalizer][INFO] - Normalizing: sin_julian_day is not normalized.
[2025-12-15 19:36:14,601][anemoi.models.preprocessing.normalizer][INFO] - Normalizing: sin_latitude is not normalized.
[2025-12-15 19:36:14,601][anemoi.models.preprocessing.normalizer][INFO] - Normalizing: sin_local_time is not normalized.
[2025-12-15 19:36:14,601][anemoi.models.preprocessing.normalizer][INFO] - Normalizing: sin_longitude is not normalized.
/lus/h2resw01/hpcperm/nld4584/anemoi_core_2025_17_11/anemoi-utils/src/anemoi/utils/config.py:207: UserWarning: Mofifying and instance of DotDict(). This class is intended to be immutable.
  warnings.warn("Mofifying and instance of DotDict(). This class is intended to be immutable.")
[2025-12-15 19:36:15,131][anemoi.models.layers.utils][INFO] - Linear kernel: torch.nn.Linear.
[2025-12-15 19:36:15,132][anemoi.models.layers.utils][INFO] - LayerNorm kernel: torch.nn.LayerNorm.
[2025-12-15 19:36:15,132][anemoi.models.layers.utils][INFO] - Activation kernel: torch.nn.GELU.
[2025-12-15 19:36:15,213][anemoi.models.layers.utils][INFO] - QueryNorm kernel: anemoi.models.layers.normalization.AutocastLayerNorm.
[2025-12-15 19:36:15,214][anemoi.models.layers.utils][INFO] - KeyNorm kernel: anemoi.models.layers.normalization.AutocastLayerNorm.
[2025-12-15 19:36:19,040][anemoi.models.layers.utils][INFO] - Linear kernel: torch.nn.Linear.
[2025-12-15 19:36:19,040][anemoi.models.layers.utils][INFO] - LayerNorm kernel: torch.nn.LayerNorm.
[2025-12-15 19:36:19,041][anemoi.models.layers.utils][INFO] - Activation kernel: torch.nn.GELU.
[2025-12-15 19:36:19,042][anemoi.models.layers.utils][INFO] - QueryNorm kernel: anemoi.models.layers.normalization.AutocastLayerNorm.
[2025-12-15 19:36:19,042][anemoi.models.layers.utils][INFO] - KeyNorm kernel: anemoi.models.layers.normalization.AutocastLayerNorm.
[2025-12-15 19:36:19,043][anemoi.models.layers.attention][INFO] - Using flash_attention
[2025-12-15 19:36:22,345][anemoi.models.layers.attention][INFO] - Using flash_attention
[2025-12-15 19:36:22,414][anemoi.models.layers.attention][INFO] - Using flash_attention
[2025-12-15 19:36:22,483][anemoi.models.layers.attention][INFO] - Using flash_attention
[2025-12-15 19:36:22,552][anemoi.models.layers.attention][INFO] - Using flash_attention
[2025-12-15 19:36:22,621][anemoi.models.layers.attention][INFO] - Using flash_attention
[2025-12-15 19:36:22,690][anemoi.models.layers.attention][INFO] - Using flash_attention
[2025-12-15 19:36:22,759][anemoi.models.layers.attention][INFO] - Using flash_attention
[2025-12-15 19:36:22,828][anemoi.models.layers.attention][INFO] - Using flash_attention
[2025-12-15 19:36:22,897][anemoi.models.layers.attention][INFO] - Using flash_attention
[2025-12-15 19:36:22,966][anemoi.models.layers.attention][INFO] - Using flash_attention
[2025-12-15 19:36:23,035][anemoi.models.layers.attention][INFO] - Using flash_attention
[2025-12-15 19:36:23,104][anemoi.models.layers.attention][INFO] - Using flash_attention
[2025-12-15 19:36:23,173][anemoi.models.layers.attention][INFO] - Using flash_attention
[2025-12-15 19:36:23,242][anemoi.models.layers.attention][INFO] - Using flash_attention
[2025-12-15 19:36:23,310][anemoi.models.layers.attention][INFO] - Using flash_attention
[2025-12-15 19:36:23,385][anemoi.models.layers.utils][INFO] - Linear kernel: torch.nn.Linear.
[2025-12-15 19:36:23,385][anemoi.models.layers.utils][INFO] - LayerNorm kernel: torch.nn.LayerNorm.
[2025-12-15 19:36:23,386][anemoi.models.layers.utils][INFO] - Activation kernel: torch.nn.GELU.
[2025-12-15 19:36:23,387][anemoi.models.layers.utils][INFO] - QueryNorm kernel: anemoi.models.layers.normalization.AutocastLayerNorm.
[2025-12-15 19:36:23,387][anemoi.models.layers.utils][INFO] - KeyNorm kernel: anemoi.models.layers.normalization.AutocastLayerNorm.
[2025-12-15 19:36:23,871][anemoi.training.losses.scalers.variable_level][INFO] - Variable Level Scaling: Applying ReluVariableLevelScaler scaling to pl variables ({'param': ['q', 't', 'u', 'v', 'w', 'z']})
[2025-12-15 19:36:23,872][anemoi.training.losses.scalers.variable_level][INFO] - with slope = 0.001 and y-intercept/minimum = 0.2.
[2025-12-15 19:36:24,205][anemoi.training.train.train][INFO] - The following submodules will NOT be trained: []
[2025-12-15 19:36:25,534][anemoi.training.train.train][INFO] - Resuming training from last checkpoint: /ec/res4/hpcperm/nld4584/Anemoi_S2S_experiment/checkpoints_mariana/louca/last.ckpt
Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/16
[2025-12-15 20:06:29,847][anemoi.training.diagnostics.mlflow.logger][INFO] - Stopping terminal log monitoring and saving buffered terminal outputs. Final status: FAILED
Error executing job with overrides: []
Traceback (most recent call last):
  File "/lus/h2resw01/hpcperm/nld4584/anemoi_core_2025_17_11/anemoi-core/training/src/anemoi/training/train/train.py", line 538, in main
    AnemoiTrainer(config).train()
  File "/lus/h2resw01/hpcperm/nld4584/anemoi_core_2025_17_11/anemoi-core/training/src/anemoi/training/train/train.py", line 524, in train
    trainer.fit(
  File "/lus/h2resw01/hpcperm/nld4584/anemoi_core_2025_17_11/venv_anemoi_core_2025_17_11/lib/python3.11/site-packages/pytorch_lightning/trainer/trainer.py", line 561, in fit
    call._call_and_handle_interrupt(
  File "/lus/h2resw01/hpcperm/nld4584/anemoi_core_2025_17_11/venv_anemoi_core_2025_17_11/lib/python3.11/site-packages/pytorch_lightning/trainer/call.py", line 47, in _call_and_handle_interrupt
    return trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/h2resw01/hpcperm/nld4584/anemoi_core_2025_17_11/venv_anemoi_core_2025_17_11/lib/python3.11/site-packages/pytorch_lightning/strategies/launchers/subprocess_script.py", line 105, in launch
    return function(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/h2resw01/hpcperm/nld4584/anemoi_core_2025_17_11/venv_anemoi_core_2025_17_11/lib/python3.11/site-packages/pytorch_lightning/trainer/trainer.py", line 599, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "/lus/h2resw01/hpcperm/nld4584/anemoi_core_2025_17_11/venv_anemoi_core_2025_17_11/lib/python3.11/site-packages/pytorch_lightning/trainer/trainer.py", line 968, in _run
    self.strategy.setup_environment()
  File "/lus/h2resw01/hpcperm/nld4584/anemoi_core_2025_17_11/venv_anemoi_core_2025_17_11/lib/python3.11/site-packages/pytorch_lightning/strategies/ddp.py", line 154, in setup_environment
    self.setup_distributed()
  File "/lus/h2resw01/hpcperm/nld4584/anemoi_core_2025_17_11/venv_anemoi_core_2025_17_11/lib/python3.11/site-packages/pytorch_lightning/strategies/ddp.py", line 203, in setup_distributed
    _init_dist_connection(self.cluster_environment, self._process_group_backend, timeout=self._timeout)
  File "/lus/h2resw01/hpcperm/nld4584/anemoi_core_2025_17_11/venv_anemoi_core_2025_17_11/lib/python3.11/site-packages/lightning_fabric/utilities/distributed.py", line 298, in _init_dist_connection
    torch.distributed.init_process_group(torch_distributed_backend, rank=global_rank, world_size=world_size, **kwargs)
  File "/lus/h2resw01/hpcperm/nld4584/anemoi_core_2025_17_11/venv_anemoi_core_2025_17_11/lib/python3.11/site-packages/torch/distributed/c10d_logger.py", line 81, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/lus/h2resw01/hpcperm/nld4584/anemoi_core_2025_17_11/venv_anemoi_core_2025_17_11/lib/python3.11/site-packages/torch/distributed/c10d_logger.py", line 95, in wrapper
    func_return = func(*args, **kwargs)
                  ^^^^^^^^^^^^^^^^^^^^^
  File "/lus/h2resw01/hpcperm/nld4584/anemoi_core_2025_17_11/venv_anemoi_core_2025_17_11/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py", line 1714, in init_process_group
    store, rank, world_size = next(rendezvous_iterator)
                              ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/h2resw01/hpcperm/nld4584/anemoi_core_2025_17_11/venv_anemoi_core_2025_17_11/lib/python3.11/site-packages/torch/distributed/rendezvous.py", line 274, in _env_rendezvous_handler
    store = _create_c10d_store(
            ^^^^^^^^^^^^^^^^^^^
  File "/lus/h2resw01/hpcperm/nld4584/anemoi_core_2025_17_11/venv_anemoi_core_2025_17_11/lib/python3.11/site-packages/torch/distributed/rendezvous.py", line 194, in _create_c10d_store
    return TCPStore(
           ^^^^^^^^^
torch.distributed.DistStoreError: Timed out after 1801 seconds waiting for clients. 1/16 clients joined.

Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
[ECMWF-INFO -ecepilog] ----------------------------------------------------------------------------------------------------
[ECMWF-INFO -ecepilog] This is the ECMWF job Epilogue
[ECMWF-INFO -ecepilog] +++ Please report issues using the Support portal +++
[ECMWF-INFO -ecepilog] +++ https://support.ecmwf.int                     +++
[ECMWF-INFO -ecepilog] ----------------------------------------------------------------------------------------------------
[ECMWF-INFO -ecepilog] Run at 2025-12-15T20:07:04 on ac
[ECMWF-INFO -ecepilog] JobName                   : finetuning_10k_default
[ECMWF-INFO -ecepilog] JobID                     : 39501466
[ECMWF-INFO -ecepilog] Submit                    : 2025-12-15T15:00:09
[ECMWF-INFO -ecepilog] Start                     : 2025-12-15T19:22:22
[ECMWF-INFO -ecepilog] End                       : 2025-12-15T20:07:04
[ECMWF-INFO -ecepilog] QueuedTime                : 15733.0
[ECMWF-INFO -ecepilog] ElapsedRaw                : 2682
[ECMWF-INFO -ecepilog] ExitCode                  : 1:0
[ECMWF-INFO -ecepilog] DerivedExitCode           : 0:0
[ECMWF-INFO -ecepilog] State                     : FAILED
[ECMWF-INFO -ecepilog] Account                   : nlcko
[ECMWF-INFO -ecepilog] QOS                       : ng
[ECMWF-INFO -ecepilog] User                      : nld4584
[ECMWF-INFO -ecepilog] StdOut                    : /ec/res4/hpcperm/nld4584/Anemoi_S2S_experiment/./slurm_scripts/output_slurm/finetuning_10k_default-39501466.out
[ECMWF-INFO -ecepilog] StdErr                    : /ec/res4/hpcperm/nld4584/Anemoi_S2S_experiment/./slurm_scripts/output_slurm/finetuning_10k_default-39501466.out
[ECMWF-INFO -ecepilog] NNodes                    : 4
[ECMWF-INFO -ecepilog] NCPUS                     : 512
[ECMWF-INFO -ecepilog] SBU                       : 3254.383
[ECMWF-INFO -ecepilog] jobtag                    : nld4584-finetuning_0k_default-4x512-/ec/res4/hpcperm/nld4584/Anemoi_S2S_experiment/./slurm_scripts/output_slurm/finetuning_10k_default-_JOBID_.out
[ECMWF-INFO -ecepilog] ----------------------------------------------------------------------------------------------------

