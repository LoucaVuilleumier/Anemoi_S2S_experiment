[ECMWF-INFO -sbatch] - -------------------------------------------------------------------------------------
[ECMWF-INFO -sbatch] -  This is the ECMWF jobfilter
[ECMWF-INFO -sbatch] -  +++ Please report issues using the Support portal +++
[ECMWF-INFO -sbatch] -  +++ https://support.ecmwf.int                     +++
[ECMWF-INFO -sbatch] -  /usr/local/bin/ecsbatch: size: 53801, mtime: Tue Nov 25 12:31:30 2025
[ECMWF-INFO -sbatch] - -------------------------------------------------------------------------------------
[ECMWF-INFO -sbatch] - Time at submit: Wed Dec 10 14:32:05 2025 (1765377125.6012404) on ac6-102.bullx:/lus/h2resw01/hpcperm/nld4584/Anemoi_S2S_experiment/slurm_scripts
[ECMWF-INFO -sbatch] - --- SLURM VARIABLES ---
[ECMWF-INFO -sbatch] - EC_CLUSTER=ac
[ECMWF-INFO -sbatch] - SLURM_EXPORT_ENV=ALL
[ECMWF-INFO -sbatch] - SBATCH_EXPORT=NONE
[ECMWF-INFO -sbatch] - -----------------------
[ECMWF-INFO -sbatch] - jobscript: /lus/h2resw01/hpcperm/nld4584/Anemoi_S2S_experiment/slurm_scripts/test_warm_up.sh
[ECMWF-INFO -sbatch] - --- SCRIPT OPTIONS ---
[ECMWF-INFO -sbatch] - #SBATCH --job-name=test_job
[ECMWF-INFO -sbatch] - #SBATCH --output=./slurm_scripts/output_slurm/hello-%J.out
[ECMWF-INFO -sbatch] - #SBATCH --error=./slurm_scripts/output_slurm/hello-%J.out
[ECMWF-INFO -sbatch] - #SBATCH --chdir=/ec/res4/hpcperm/nld4584/Anemoi_S2S_experiment
[ECMWF-INFO -sbatch] - #SBATCH --qos=ng
[ECMWF-INFO -sbatch] - #SBATCH --time=01:00:00
[ECMWF-INFO -sbatch] - #SBATCH --partition=gpu
[ECMWF-INFO -sbatch] - #SBATCH --gres=gpu:1
[ECMWF-INFO -sbatch] - #SBATCH --mem=60G
[ECMWF-INFO -sbatch] - -----------------------
[ECMWF-INFO -sbatch] - --- POST-PROCESSED OPTIONS ---
[ECMWF-INFO -sbatch] - ARG --positional=['./test_warm_up.sh']
[ECMWF-INFO -sbatch] - ARG --chdir=/ec/res4/hpcperm/nld4584/Anemoi_S2S_experiment
[ECMWF-INFO -sbatch] - ARG --error=./slurm_scripts/output_slurm/hello-%J.out
[ECMWF-INFO -sbatch] - ARG --gres=gpu:1
[ECMWF-INFO -sbatch] - ARG --job_name=test_job
[ECMWF-INFO -sbatch] - ARG --output=./slurm_scripts/output_slurm/hello-%J.out
[ECMWF-INFO -sbatch] - ARG --partition=gpu
[ECMWF-INFO -sbatch] - ARG --qos=ng
[ECMWF-INFO -sbatch] - ARG --time=01:00:00
[ECMWF-INFO -sbatch] - ARG --mem=60G
[ECMWF-INFO -sbatch] - ------------------------------
[ECMWF-INFO -sbatch] - jobtag: nld4584-test_job-1x2-/lus/h2resw01/hpcperm/nld4584/Anemoi_S2S_experiment/slurm_scripts/./slurm_scripts/output_slurm/hello-_.out
[ECMWF-INFO -sbatch] - ['/usr/bin/sbatch', '--chdir=/ec/res4/hpcperm/nld4584/Anemoi_S2S_experiment', '--error=./slurm_scripts/output_slurm/hello-%J.out', '--gres=gpu:1', '--job-name=test_job', '--output=./slurm_scripts/output_slurm/hello-%J.out', '--partition=gpu', '--qos=ng', '--time=01:00:00', '--mem=60G', '--licenses=h2resw01', '--export=EC_user_time_limit=01:00:00', '/lus/h2resw01/hpcperm/nld4584/Anemoi_S2S_experiment/slurm_scripts/test_warm_up.sh']
[ECMWF-INFO -sbatch] - sbatch executed on ac
[ECMWF-INFO -sbatch] - Job queued on ac using method local
[ECMWF-INFO -sbatch] - Submitted batch job 36910671
[ECMWF-INFO -ecprofile] /usr/bin/bash NON_INTERACTIVE on ac6-314 at 20251210_143210.713, PID: 3398926, JOBID: 36910671
[ECMWF-INFO -ecprofile] $SCRATCH=/ec/res4/scratch/nld4584
[ECMWF-INFO -ecprofile] $PERM=/perm/nld4584
[ECMWF-INFO -ecprofile] $HPCPERM=/ec/res4/hpcperm/nld4584
[ECMWF-INFO -ecprofile] $TMPDIR=/dev/shm/_tmpdir_.nld4584.36910671
[ECMWF-INFO -ecprofile] $SCRATCHDIR=/ec/res4/scratchdir/nld4584/7/36910671
2025-12-10 14:42:03 INFO Running anemoi training command with overrides: ['--config-name=test_warm_up.yaml']
2025-12-10 14:43:56 INFO Prepending Anemoi Config Env (/ec/res4/hpcperm/nld4584/Anemoi_S2S_experiment/Configs_new/Training) to the search path.
2025-12-10 14:43:56 INFO Prepending current user directory (/lus/h2resw01/hpcperm/nld4584/Anemoi_S2S_experiment) to the search path.
2025-12-10 14:43:56 INFO Search path is now: [provider=anemoi-cwd-searchpath-plugin, path=/lus/h2resw01/hpcperm/nld4584/Anemoi_S2S_experiment, provider=anemoi-env-searchpath-plugin, path=/ec/res4/hpcperm/nld4584/Anemoi_S2S_experiment/Configs_new/Training, provider=hydra, path=pkg://hydra.conf, provider=main, path=pkg://anemoi.training/config]
[2025-12-10 14:44:02,058][anemoi.training.train.train][INFO] - Skipping config validation.
[2025-12-10 14:44:02,059][anemoi.training.train.train][INFO] - Starting from checkpoint: False
[2025-12-10 14:44:02,059][anemoi.training.train.train][INFO] - Run id: fdc4cf41-3ee2-43d0-946c-b63fbb17cb9c
[2025-12-10 14:44:02,060][anemoi.training.train.train][INFO] - Checkpoints path: output_training/checkpoint/fdc4cf41-3ee2-43d0-946c-b63fbb17cb9c
[2025-12-10 14:44:02,060][anemoi.training.train.train][INFO] - Plots path: output_training/plots/fdc4cf41-3ee2-43d0-946c-b63fbb17cb9c
/lus/h2resw01/hpcperm/nld4584/anemoi_core_2025_17_11/anemoi-utils/src/anemoi/utils/config.py:207: UserWarning: Mofifying and instance of DotDict(). This class is intended to be immutable.
  warnings.warn("Mofifying and instance of DotDict(). This class is intended to be immutable.")
[2025-12-10 14:45:21,350][anemoi.graphs.nodes.builders.from_file][INFO] - Reading the dataset from /home/mlx/ai-ml/datasets//aifs-ea-an-oper-0001-mars-o96-1979-2023-6h-v8.zarr.
[2025-12-10 14:45:29,382][anemoi.utils.config][INFO] - Using environment variable ANEMOI_CONFIG_PATH to override the anemoi config key 'path.'
[2025-12-10 14:45:44,460][anemoi.graphs.edges.builders.cutoff][INFO] - Using CutOff-Edges (with radius = 144.6 km) between data and hidden.
[2025-12-10 14:45:46,433][anemoi.graphs.edges.builders.base][WARNING] - The 'torch-cluster' library is not installed. Installing 'torch-cluster' can significantly improve performance for graph creation. You can install it using 'pip install torch-cluster'.
[2025-12-10 14:45:58,046][anemoi.graphs.edges.builders.knn][INFO] - Using KNN-Edges (with 3 nearest neighbours) between hidden and data.
[2025-12-10 14:45:58,046][anemoi.graphs.edges.builders.base][WARNING] - The 'torch-cluster' library is not installed. Installing 'torch-cluster' can significantly improve performance for graph creation. You can install it using 'pip install torch-cluster'.
[2025-12-10 14:45:58,115][anemoi.graphs.create][INFO] - Cleaning graph.
[2025-12-10 14:45:58,115][anemoi.graphs.create][INFO] - _grid_reference_distance deleted from graph.
[2025-12-10 14:45:58,115][anemoi.graphs.create][INFO] - _dataset deleted from graph.
[2025-12-10 14:45:58,115][anemoi.graphs.create][INFO] - _grid_reference_distance deleted from graph.
[2025-12-10 14:45:58,484][anemoi.graphs.create][INFO] - Graph saved at graphs/graph.pkl.
[2025-12-10 14:45:59,725][anemoi.training.data.datamodule.singledatamodule][WARNING] - Falling back rollout to: 1
[2025-12-10 14:45:59,725][anemoi.training.data.datamodule.singledatamodule][INFO] - Timeincrement set to 1 for data with frequency, 21600, and timestep, 21600
[2025-12-10 14:45:59,728][anemoi.training.train.train][INFO] - Number of data variables: 101
[2025-12-10 14:45:59,728][anemoi.training.train.train][INFO] - Training limits: max_epochs=2, max_steps=300. Training will stop when either limit is reached first. Learning rate scheduler will run for 300000 steps.
[2025-12-10 14:47:30,194][anemoi.training.diagnostics.callbacks.plot][INFO] - Setting up asynchronous plotting ...
[2025-12-10 14:47:30,196][anemoi.training.diagnostics.callbacks.plot][INFO] - Setting up asynchronous plotting ...
[2025-12-10 14:47:30,236][anemoi.training.diagnostics.callbacks.plot][INFO] - Setting up asynchronous plotting ...
[2025-12-10 14:47:30,236][anemoi.training.diagnostics.callbacks.plot][INFO] - Using defined accumulation colormap for fields: ['tp', 'cp']
[2025-12-10 14:47:30,238][anemoi.training.diagnostics.callbacks.plot][INFO] - Setting up asynchronous plotting ...
[2025-12-10 14:47:30,239][anemoi.training.diagnostics.callbacks.plot][INFO] - Setting up asynchronous plotting ...
[2025-12-10 14:47:30,240][anemoi.training.diagnostics.callbacks.plot][INFO] - Using precip histogram plotting method for fields: ['tp', 'cp'].
/lus/h2resw01/hpcperm/nld4584/anemoi_core_2025_17_11/venv_anemoi_core_2025_17_11/lib/python3.11/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python3 /lus/h2resw01/hpcperm/nld4584/anemoi_core_2025_17_1 ...
Using 16bit Automatic Mixed Precision (AMP)
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
[rank: 0] Seed set to 36910671
/lus/h2resw01/hpcperm/nld4584/anemoi_core_2025_17_11/anemoi-utils/src/anemoi/utils/provenance.py:143: UserWarning: The '__version__' attribute is deprecated and will be removed in MarkupSafe 3.1. Use feature detection, or `importlib.metadata.version("markupsafe")`, instead.
  versions[name] = str(module.__version__)
[2025-12-10 14:49:02,584][anemoi.models.preprocessing.normalizer][INFO] - Normalizing: cos_julian_day is not normalized.
[2025-12-10 14:49:02,587][anemoi.models.preprocessing.normalizer][INFO] - Normalizing: cos_latitude is not normalized.
[2025-12-10 14:49:02,587][anemoi.models.preprocessing.normalizer][INFO] - Normalizing: cos_local_time is not normalized.
[2025-12-10 14:49:02,587][anemoi.models.preprocessing.normalizer][INFO] - Normalizing: cos_longitude is not normalized.
[2025-12-10 14:49:02,587][anemoi.models.preprocessing.normalizer][INFO] - Normalizing: insolation is not normalized.
[2025-12-10 14:49:02,587][anemoi.models.preprocessing.normalizer][INFO] - Normalizing: lsm is not normalized.
[2025-12-10 14:49:02,587][anemoi.models.preprocessing.normalizer][INFO] - Normalizing: sin_julian_day is not normalized.
[2025-12-10 14:49:02,587][anemoi.models.preprocessing.normalizer][INFO] - Normalizing: sin_latitude is not normalized.
[2025-12-10 14:49:02,587][anemoi.models.preprocessing.normalizer][INFO] - Normalizing: sin_local_time is not normalized.
[2025-12-10 14:49:02,587][anemoi.models.preprocessing.normalizer][INFO] - Normalizing: sin_longitude is not normalized.
/lus/h2resw01/hpcperm/nld4584/anemoi_core_2025_17_11/anemoi-utils/src/anemoi/utils/config.py:207: UserWarning: Mofifying and instance of DotDict(). This class is intended to be immutable.
  warnings.warn("Mofifying and instance of DotDict(). This class is intended to be immutable.")
[2025-12-10 14:49:03,199][anemoi.models.layers.utils][INFO] - Linear kernel: torch.nn.Linear.
[2025-12-10 14:49:03,200][anemoi.models.layers.utils][INFO] - LayerNorm kernel: torch.nn.LayerNorm.
[2025-12-10 14:49:03,201][anemoi.models.layers.utils][INFO] - Activation kernel: torch.nn.GELU.
[2025-12-10 14:49:03,452][anemoi.models.layers.utils][INFO] - QueryNorm kernel: anemoi.models.layers.normalization.AutocastLayerNorm.
[2025-12-10 14:49:03,453][anemoi.models.layers.utils][INFO] - KeyNorm kernel: anemoi.models.layers.normalization.AutocastLayerNorm.
[2025-12-10 14:49:03,843][anemoi.models.layers.utils][INFO] - Linear kernel: torch.nn.Linear.
[2025-12-10 14:49:03,843][anemoi.models.layers.utils][INFO] - LayerNorm kernel: torch.nn.LayerNorm.
[2025-12-10 14:49:03,844][anemoi.models.layers.utils][INFO] - Activation kernel: torch.nn.GELU.
[2025-12-10 14:49:03,845][anemoi.models.layers.utils][INFO] - QueryNorm kernel: anemoi.models.layers.normalization.AutocastLayerNorm.
[2025-12-10 14:49:03,845][anemoi.models.layers.utils][INFO] - KeyNorm kernel: anemoi.models.layers.normalization.AutocastLayerNorm.
[2025-12-10 14:49:03,845][anemoi.models.layers.attention][INFO] - Using flash_attention
[2025-12-10 14:49:05,152][anemoi.models.layers.attention][INFO] - Using flash_attention
[2025-12-10 14:49:05,220][anemoi.models.layers.attention][INFO] - Using flash_attention
[2025-12-10 14:49:05,288][anemoi.models.layers.attention][INFO] - Using flash_attention
[2025-12-10 14:49:05,356][anemoi.models.layers.attention][INFO] - Using flash_attention
[2025-12-10 14:49:05,423][anemoi.models.layers.attention][INFO] - Using flash_attention
[2025-12-10 14:49:05,490][anemoi.models.layers.attention][INFO] - Using flash_attention
[2025-12-10 14:49:05,558][anemoi.models.layers.attention][INFO] - Using flash_attention
[2025-12-10 14:49:05,625][anemoi.models.layers.attention][INFO] - Using flash_attention
[2025-12-10 14:49:05,692][anemoi.models.layers.attention][INFO] - Using flash_attention
[2025-12-10 14:49:05,759][anemoi.models.layers.attention][INFO] - Using flash_attention
[2025-12-10 14:49:05,826][anemoi.models.layers.attention][INFO] - Using flash_attention
[2025-12-10 14:49:05,893][anemoi.models.layers.attention][INFO] - Using flash_attention
[2025-12-10 14:49:05,961][anemoi.models.layers.attention][INFO] - Using flash_attention
[2025-12-10 14:49:06,028][anemoi.models.layers.attention][INFO] - Using flash_attention
[2025-12-10 14:49:06,095][anemoi.models.layers.attention][INFO] - Using flash_attention
[2025-12-10 14:49:06,167][anemoi.models.layers.utils][INFO] - Linear kernel: torch.nn.Linear.
[2025-12-10 14:49:06,168][anemoi.models.layers.utils][INFO] - LayerNorm kernel: torch.nn.LayerNorm.
[2025-12-10 14:49:06,169][anemoi.models.layers.utils][INFO] - Activation kernel: torch.nn.GELU.
[2025-12-10 14:49:06,169][anemoi.models.layers.utils][INFO] - QueryNorm kernel: anemoi.models.layers.normalization.AutocastLayerNorm.
[2025-12-10 14:49:06,170][anemoi.models.layers.utils][INFO] - KeyNorm kernel: anemoi.models.layers.normalization.AutocastLayerNorm.
[2025-12-10 14:49:06,578][anemoi.training.losses.scalers.variable_level][INFO] - Variable Level Scaling: Applying ReluVariableLevelScaler scaling to pl variables ({'param': ['q', 't', 'u', 'v', 'w', 'z']})
[2025-12-10 14:49:06,578][anemoi.training.losses.scalers.variable_level][INFO] - with slope = 0.001 and y-intercept/minimum = 0.2.
[2025-12-10 14:49:06,997][anemoi.training.train.train][INFO] - The following submodules will NOT be trained: []
/lus/h2resw01/hpcperm/nld4584/anemoi_core_2025_17_11/venv_anemoi_core_2025_17_11/lib/python3.11/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python3 /lus/h2resw01/hpcperm/nld4584/anemoi_core_2025_17_1 ...
Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/1
----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All distributed processes registered. Starting with 1 processes
----------------------------------------------------------------------------------------------------

LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[rank: 0] Seed set to 36910671

  | Name    | Type                 | Params | Mode 
---------------------------------------------------------
0 | model   | AnemoiModelInterface | 231 M  | train
1 | loss    | MSELoss              | 0      | train
2 | metrics | ModuleDict           | 0      | train
---------------------------------------------------------
231 M     Trainable params
0         Non-trainable params
231 M     Total params
924.556   Total estimated model params size (MB)
278       Modules in train mode
0         Modules in eval mode
Sanity Checking: |          | 0/? [00:00<?, ?it/s][2025-12-10 14:50:23,366][anemoi.training.data.datamodule.singledatamodule][WARNING] - Falling back rollout to: 1
/lus/h2resw01/hpcperm/nld4584/anemoi_core_2025_17_11/venv_anemoi_core_2025_17_11/lib/python3.11/site-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
[2025-12-10 14:50:23,836][anemoi.models.data_indices.collection][INFO] - The order of the variables in the model matches the order in the data.
[2025-12-10 14:50:23,893][anemoi.training.data.dataset.singledataset][INFO] - Worker 5 (pid 3426020, global_rank 0, model comm group 0)  has low/high range 910 / 1092
[2025-12-10 14:50:23,893][anemoi.training.data.dataset.singledataset][INFO] - Worker 4 (pid 3426016, global_rank 0, model comm group 0)  has low/high range 728 / 910
[2025-12-10 14:50:23,894][anemoi.training.data.dataset.singledataset][INFO] - Worker 1 (pid 3426009, global_rank 0, model comm group 0)  has low/high range 182 / 364
[2025-12-10 14:50:23,894][anemoi.training.data.dataset.singledataset][INFO] - Worker 3 (pid 3426014, global_rank 0, model comm group 0)  has low/high range 546 / 728
[2025-12-10 14:50:23,896][anemoi.training.data.dataset.singledataset][INFO] - Worker 0 (pid 3426008, global_rank 0, model comm group 0)  has low/high range 0 / 182
[2025-12-10 14:50:23,896][anemoi.training.data.dataset.singledataset][INFO] - Worker 7 (pid 3426026, global_rank 0, model comm group 0)  has low/high range 1274 / 1456
[2025-12-10 14:50:23,897][anemoi.training.data.dataset.singledataset][INFO] - Worker 6 (pid 3426024, global_rank 0, model comm group 0)  has low/high range 1092 / 1274
[2025-12-10 14:50:23,898][anemoi.training.data.dataset.singledataset][INFO] - Worker 2 (pid 3426012, global_rank 0, model comm group 0)  has low/high range 364 / 546
[2025-12-10 14:50:24,012][anemoi.training.data.dataset.singledataset][INFO] - Worker 2 (validation, pid 3426012, glob. rank 0, model comm group 0, group_rank 0, seed group id 0, base_seed 36910671, sanity rnd 0.008874)
[2025-12-10 14:50:24,012][anemoi.training.data.dataset.singledataset][INFO] - Worker 3 (validation, pid 3426014, glob. rank 0, model comm group 0, group_rank 0, seed group id 0, base_seed 36910671, sanity rnd 0.008874)
[2025-12-10 14:50:24,013][anemoi.training.data.dataset.singledataset][INFO] - Worker 6 (validation, pid 3426024, glob. rank 0, model comm group 0, group_rank 0, seed group id 0, base_seed 36910671, sanity rnd 0.008874)
[2025-12-10 14:50:24,013][anemoi.training.data.dataset.singledataset][INFO] - Worker 5 (validation, pid 3426020, glob. rank 0, model comm group 0, group_rank 0, seed group id 0, base_seed 36910671, sanity rnd 0.008874)
[2025-12-10 14:50:24,014][anemoi.training.data.dataset.singledataset][INFO] - Worker 7 (validation, pid 3426026, glob. rank 0, model comm group 0, group_rank 0, seed group id 0, base_seed 36910671, sanity rnd 0.008874)
[2025-12-10 14:50:24,014][anemoi.training.data.dataset.singledataset][INFO] - Worker 0 (validation, pid 3426008, glob. rank 0, model comm group 0, group_rank 0, seed group id 0, base_seed 36910671, sanity rnd 0.008874)
[2025-12-10 14:50:24,015][anemoi.training.data.dataset.singledataset][INFO] - Worker 1 (validation, pid 3426009, glob. rank 0, model comm group 0, group_rank 0, seed group id 0, base_seed 36910671, sanity rnd 0.008874)
[2025-12-10 14:50:24,016][anemoi.training.data.dataset.singledataset][INFO] - Worker 4 (validation, pid 3426016, glob. rank 0, model comm group 0, group_rank 0, seed group id 0, base_seed 36910671, sanity rnd 0.008874)
/lus/h2resw01/hpcperm/nld4584/anemoi_core_2025_17_11/venv_anemoi_core_2025_17_11/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:369: You have overridden `on_after_batch_transfer` in `LightningModule` but have passed in a `LightningDataModule`. It will use the implementation from `LightningModule` instance.
Sanity Checking:   0%|          | 0/6 [00:00<?, ?it/s]Sanity Checking DataLoader 0:   0%|          | 0/6 [00:00<?, ?it/s]Sanity Checking DataLoader 0:  17%|█▋        | 1/6 [00:09<00:49,  0.10it/s]Sanity Checking DataLoader 0:  33%|███▎      | 2/6 [00:13<00:26,  0.15it/s]Sanity Checking DataLoader 0:  50%|█████     | 3/6 [00:14<00:14,  0.21it/s]Sanity Checking DataLoader 0:  67%|██████▋   | 4/6 [00:14<00:07,  0.27it/s]Sanity Checking DataLoader 0:  83%|████████▎ | 5/6 [00:15<00:03,  0.32it/s]Sanity Checking DataLoader 0: 100%|██████████| 6/6 [00:16<00:00,  0.37it/s]                                                                           /lus/h2resw01/hpcperm/nld4584/anemoi_core_2025_17_11/venv_anemoi_core_2025_17_11/lib/python3.11/site-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
[2025-12-10 14:50:45,764][anemoi.training.data.dataset.singledataset][INFO] - Worker 0 (pid 3426174, global_rank 0, model comm group 0)  has low/high range 0 / 7670
[2025-12-10 14:50:45,772][anemoi.training.data.dataset.singledataset][INFO] - Worker 1 (pid 3426176, global_rank 0, model comm group 0)  has low/high range 7670 / 15340
[2025-12-10 14:50:45,775][anemoi.training.data.dataset.singledataset][INFO] - Worker 0 (train, pid 3426174, glob. rank 0, model comm group 0, group_rank 0, seed group id 0, base_seed 36910671, sanity rnd 0.008874)
[2025-12-10 14:50:45,776][anemoi.training.data.dataset.singledataset][INFO] - Worker 1 (train, pid 3426176, glob. rank 0, model comm group 0, group_rank 0, seed group id 0, base_seed 36910671, sanity rnd 0.008874)
[2025-12-10 14:50:45,933][anemoi.training.data.dataset.singledataset][INFO] - Worker 2 (pid 3426188, global_rank 0, model comm group 0)  has low/high range 15340 / 23010
[2025-12-10 14:50:45,936][anemoi.training.data.dataset.singledataset][INFO] - Worker 2 (train, pid 3426188, glob. rank 0, model comm group 0, group_rank 0, seed group id 0, base_seed 36910671, sanity rnd 0.008874)
[2025-12-10 14:50:46,051][anemoi.training.data.dataset.singledataset][INFO] - Worker 3 (pid 3426190, global_rank 0, model comm group 0)  has low/high range 23010 / 30680
[2025-12-10 14:50:46,054][anemoi.training.data.dataset.singledataset][INFO] - Worker 3 (train, pid 3426190, glob. rank 0, model comm group 0, group_rank 0, seed group id 0, base_seed 36910671, sanity rnd 0.008874)
[2025-12-10 14:50:46,131][anemoi.training.data.dataset.singledataset][INFO] - Worker 4 (pid 3426192, global_rank 0, model comm group 0)  has low/high range 30680 / 38350
[2025-12-10 14:50:46,134][anemoi.training.data.dataset.singledataset][INFO] - Worker 4 (train, pid 3426192, glob. rank 0, model comm group 0, group_rank 0, seed group id 0, base_seed 36910671, sanity rnd 0.008874)
[2025-12-10 14:50:46,199][anemoi.training.data.dataset.singledataset][INFO] - Worker 5 (pid 3426200, global_rank 0, model comm group 0)  has low/high range 38350 / 46020
[2025-12-10 14:50:46,202][anemoi.training.data.dataset.singledataset][INFO] - Worker 5 (train, pid 3426200, glob. rank 0, model comm group 0, group_rank 0, seed group id 0, base_seed 36910671, sanity rnd 0.008874)
[2025-12-10 14:50:46,292][anemoi.training.data.dataset.singledataset][INFO] - Worker 7 (pid 3426204, global_rank 0, model comm group 0)  has low/high range 53690 / 61360
[2025-12-10 14:50:46,294][anemoi.training.data.dataset.singledataset][INFO] - Worker 7 (train, pid 3426204, glob. rank 0, model comm group 0, group_rank 0, seed group id 0, base_seed 36910671, sanity rnd 0.008874)
[2025-12-10 14:50:46,301][anemoi.training.data.dataset.singledataset][INFO] - Worker 6 (pid 3426202, global_rank 0, model comm group 0)  has low/high range 46020 / 53690
[2025-12-10 14:50:46,304][anemoi.training.data.dataset.singledataset][INFO] - Worker 6 (train, pid 3426202, glob. rank 0, model comm group 0, group_rank 0, seed group id 0, base_seed 36910671, sanity rnd 0.008874)
[2025-12-10 14:50:49,124][anemoi.models.data_indices.collection][INFO] - The order of the variables in the model matches the order in the data.
Training: |          | 0/? [00:00<?, ?it/s]/lus/h2resw01/hpcperm/nld4584/anemoi_core_2025_17_11/anemoi-utils/src/anemoi/utils/config.py:207: UserWarning: Mofifying and instance of DotDict(). This class is intended to be immutable.
  warnings.warn("Mofifying and instance of DotDict(). This class is intended to be immutable.")
Training: |          | 0/? [00:00<?, ?it/s]Epoch 0: |          | 0/? [00:00<?, ?it/s] Epoch 0: |          | 1/? [00:11<00:00,  0.09it/s]Epoch 0: |          | 1/? [00:11<00:00,  0.09it/s, train_mse_loss_step=0.623]Epoch 0: |          | 2/? [00:11<00:00,  0.17it/s, train_mse_loss_step=0.623]Epoch 0: |          | 2/? [00:11<00:00,  0.17it/s, train_mse_loss_step=0.591]Epoch 0: |          | 3/? [00:12<00:00,  0.24it/s, train_mse_loss_step=0.591]Epoch 0: |          | 3/? [00:12<00:00,  0.24it/s, train_mse_loss_step=0.577]Epoch 0: |          | 4/? [00:12<00:00,  0.31it/s, train_mse_loss_step=0.577]Epoch 0: |          | 4/? [00:12<00:00,  0.31it/s, train_mse_loss_step=0.558]Epoch 0: |          | 5/? [00:13<00:00,  0.37it/s, train_mse_loss_step=0.558]Epoch 0: |          | 5/? [00:13<00:00,  0.37it/s, train_mse_loss_step=0.551]Epoch 0: |          | 6/? [00:13<00:00,  0.43it/s, train_mse_loss_step=0.551]Epoch 0: |          | 6/? [00:13<00:00,  0.43it/s, train_mse_loss_step=0.524]Epoch 0: |          | 7/? [00:14<00:00,  0.48it/s, train_mse_loss_step=0.524]Epoch 0: |          | 7/? [00:14<00:00,  0.48it/s, train_mse_loss_step=0.520]Epoch 0: |          | 8/? [00:15<00:00,  0.53it/s, train_mse_loss_step=0.520]Epoch 0: |          | 8/? [00:15<00:00,  0.53it/s, train_mse_loss_step=0.538]Epoch 0: |          | 9/? [00:15<00:00,  0.57it/s, train_mse_loss_step=0.538]Epoch 0: |          | 9/? [00:15<00:00,  0.57it/s, train_mse_loss_step=0.511]Epoch 0: |          | 10/? [00:16<00:00,  0.61it/s, train_mse_loss_step=0.511]Epoch 0: |          | 10/? [00:16<00:00,  0.61it/s, train_mse_loss_step=0.481]Epoch 0: |          | 11/? [00:16<00:00,  0.65it/s, train_mse_loss_step=0.481]Epoch 0: |          | 11/? [00:16<00:00,  0.65it/s, train_mse_loss_step=0.452]Epoch 0: |          | 12/? [00:17<00:00,  0.69it/s, train_mse_loss_step=0.452]Epoch 0: |          | 12/? [00:17<00:00,  0.69it/s, train_mse_loss_step=0.433]Epoch 0: |          | 13/? [00:17<00:00,  0.72it/s, train_mse_loss_step=0.433]Epoch 0: |          | 13/? [00:18<00:00,  0.72it/s, train_mse_loss_step=0.415]Epoch 0: |          | 14/? [00:18<00:00,  0.75it/s, train_mse_loss_step=0.415]Epoch 0: |          | 14/? [00:18<00:00,  0.75it/s, train_mse_loss_step=0.406]Epoch 0: |          | 15/? [00:20<00:00,  0.73it/s, train_mse_loss_step=0.406]Epoch 0: |          | 15/? [00:20<00:00,  0.73it/s, train_mse_loss_step=0.396]Epoch 0: |          | 16/? [00:23<00:00,  0.69it/s, train_mse_loss_step=0.396]Epoch 0: |          | 16/? [00:23<00:00,  0.69it/s, train_mse_loss_step=0.381]Epoch 0: |          | 17/? [00:27<00:00,  0.61it/s, train_mse_loss_step=0.381]Epoch 0: |          | 17/? [00:27<00:00,  0.61it/s, train_mse_loss_step=0.388]Epoch 0: |          | 18/? [00:28<00:00,  0.63it/s, train_mse_loss_step=0.388]Epoch 0: |          | 18/? [00:28<00:00,  0.62it/s, train_mse_loss_step=0.372]Epoch 0: |          | 19/? [00:31<00:00,  0.61it/s, train_mse_loss_step=0.372]Epoch 0: |          | 19/? [00:31<00:00,  0.61it/s, train_mse_loss_step=0.368]Epoch 0: |          | 20/? [00:32<00:00,  0.62it/s, train_mse_loss_step=0.368]Epoch 0: |          | 20/? [00:32<00:00,  0.62it/s, train_mse_loss_step=0.355]Epoch 0: |          | 21/? [00:32<00:00,  0.64it/s, train_mse_loss_step=0.355]Epoch 0: |          | 21/? [00:32<00:00,  0.64it/s, train_mse_loss_step=0.348]Epoch 0: |          | 22/? [00:33<00:00,  0.66it/s, train_mse_loss_step=0.348]Epoch 0: |          | 22/? [00:33<00:00,  0.66it/s, train_mse_loss_step=0.333]Epoch 0: |          | 23/? [00:34<00:00,  0.67it/s, train_mse_loss_step=0.333]Epoch 0: |          | 23/? [00:34<00:00,  0.67it/s, train_mse_loss_step=0.324]Epoch 0: |          | 24/? [00:34<00:00,  0.69it/s, train_mse_loss_step=0.324]Epoch 0: |          | 24/? [00:34<00:00,  0.69it/s, train_mse_loss_step=0.318]Epoch 0: |          | 25/? [00:35<00:00,  0.71it/s, train_mse_loss_step=0.318]Epoch 0: |          | 25/? [00:35<00:00,  0.71it/s, train_mse_loss_step=0.323]Epoch 0: |          | 26/? [00:35<00:00,  0.73it/s, train_mse_loss_step=0.323]Epoch 0: |          | 26/? [00:35<00:00,  0.73it/s, train_mse_loss_step=0.306]Epoch 0: |          | 27/? [00:36<00:00,  0.74it/s, train_mse_loss_step=0.306]Epoch 0: |          | 27/? [00:36<00:00,  0.74it/s, train_mse_loss_step=0.320]Epoch 0: |          | 28/? [00:36<00:00,  0.76it/s, train_mse_loss_step=0.320]Epoch 0: |          | 28/? [00:36<00:00,  0.76it/s, train_mse_loss_step=0.281]Epoch 0: |          | 29/? [00:37<00:00,  0.77it/s, train_mse_loss_step=0.281]Epoch 0: |          | 29/? [00:37<00:00,  0.77it/s, train_mse_loss_step=0.299]Epoch 0: |          | 30/? [00:38<00:00,  0.79it/s, train_mse_loss_step=0.299]Epoch 0: |          | 30/? [00:38<00:00,  0.79it/s, train_mse_loss_step=0.281]Epoch 0: |          | 31/? [00:38<00:00,  0.80it/s, train_mse_loss_step=0.281]Epoch 0: |          | 31/? [00:38<00:00,  0.80it/s, train_mse_loss_step=0.259]Epoch 0: |          | 32/? [00:39<00:00,  0.81it/s, train_mse_loss_step=0.259]Epoch 0: |          | 32/? [00:39<00:00,  0.81it/s, train_mse_loss_step=0.257]Epoch 0: |          | 33/? [00:39<00:00,  0.83it/s, train_mse_loss_step=0.257]Epoch 0: |          | 33/? [00:39<00:00,  0.83it/s, train_mse_loss_step=0.256]Epoch 0: |          | 34/? [00:40<00:00,  0.84it/s, train_mse_loss_step=0.256]Epoch 0: |          | 34/? [00:40<00:00,  0.84it/s, train_mse_loss_step=0.264]Epoch 0: |          | 35/? [00:41<00:00,  0.85it/s, train_mse_loss_step=0.264]Epoch 0: |          | 35/? [00:41<00:00,  0.85it/s, train_mse_loss_step=0.250]Epoch 0: |          | 36/? [00:41<00:00,  0.87it/s, train_mse_loss_step=0.250]Epoch 0: |          | 36/? [00:41<00:00,  0.87it/s, train_mse_loss_step=0.243]Epoch 0: |          | 37/? [00:42<00:00,  0.88it/s, train_mse_loss_step=0.243]Epoch 0: |          | 37/? [00:42<00:00,  0.88it/s, train_mse_loss_step=0.232]Epoch 0: |          | 38/? [00:42<00:00,  0.89it/s, train_mse_loss_step=0.232]Epoch 0: |          | 38/? [00:42<00:00,  0.89it/s, train_mse_loss_step=0.243]Epoch 0: |          | 39/? [00:43<00:00,  0.90it/s, train_mse_loss_step=0.243]Epoch 0: |          | 39/? [00:43<00:00,  0.90it/s, train_mse_loss_step=0.228]Epoch 0: |          | 40/? [00:43<00:00,  0.91it/s, train_mse_loss_step=0.228]Epoch 0: |          | 40/? [00:43<00:00,  0.91it/s, train_mse_loss_step=0.215]Epoch 0: |          | 41/? [00:44<00:00,  0.92it/s, train_mse_loss_step=0.215]Epoch 0: |          | 41/? [00:44<00:00,  0.92it/s, train_mse_loss_step=0.214]Epoch 0: |          | 42/? [00:44<00:00,  0.93it/s, train_mse_loss_step=0.214]Epoch 0: |          | 42/? [00:45<00:00,  0.93it/s, train_mse_loss_step=0.203]Epoch 0: |          | 43/? [00:45<00:00,  0.94it/s, train_mse_loss_step=0.203]Epoch 0: |          | 43/? [00:45<00:00,  0.94it/s, train_mse_loss_step=0.197]Epoch 0: |          | 44/? [00:46<00:00,  0.95it/s, train_mse_loss_step=0.197]Epoch 0: |          | 44/? [00:46<00:00,  0.95it/s, train_mse_loss_step=0.200]Epoch 0: |          | 45/? [00:46<00:00,  0.96it/s, train_mse_loss_step=0.200]Epoch 0: |          | 45/? [00:46<00:00,  0.96it/s, train_mse_loss_step=0.196]Epoch 0: |          | 46/? [00:47<00:00,  0.97it/s, train_mse_loss_step=0.196]Epoch 0: |          | 46/? [00:47<00:00,  0.97it/s, train_mse_loss_step=0.199]Epoch 0: |          | 47/? [00:47<00:00,  0.98it/s, train_mse_loss_step=0.199]Epoch 0: |          | 47/? [00:47<00:00,  0.98it/s, train_mse_loss_step=0.185]Epoch 0: |          | 48/? [00:48<00:00,  0.99it/s, train_mse_loss_step=0.185]Epoch 0: |          | 48/? [00:48<00:00,  0.99it/s, train_mse_loss_step=0.192]Epoch 0: |          | 49/? [00:48<00:00,  1.00it/s, train_mse_loss_step=0.192]Epoch 0: |          | 49/? [00:49<00:00,  1.00it/s, train_mse_loss_step=0.193]Epoch 0: |          | 50/? [00:49<00:00,  1.01it/s, train_mse_loss_step=0.193]Epoch 0: |          | 50/? [00:49<00:00,  1.01it/s, train_mse_loss_step=0.181]Epoch 0: |          | 51/? [00:50<00:00,  1.02it/s, train_mse_loss_step=0.181]Epoch 0: |          | 51/? [00:50<00:00,  1.02it/s, train_mse_loss_step=0.166]Epoch 0: |          | 52/? [00:50<00:00,  1.03it/s, train_mse_loss_step=0.166]Epoch 0: |          | 52/? [00:50<00:00,  1.03it/s, train_mse_loss_step=0.159]Epoch 0: |          | 53/? [00:51<00:00,  1.03it/s, train_mse_loss_step=0.159]Epoch 0: |          | 53/? [00:51<00:00,  1.03it/s, train_mse_loss_step=0.163]Epoch 0: |          | 54/? [00:51<00:00,  1.04it/s, train_mse_loss_step=0.163]Epoch 0: |          | 54/? [00:51<00:00,  1.04it/s, train_mse_loss_step=0.159]Epoch 0: |          | 55/? [00:52<00:00,  1.05it/s, train_mse_loss_step=0.159]Epoch 0: |          | 55/? [00:52<00:00,  1.05it/s, train_mse_loss_step=0.156]Epoch 0: |          | 56/? [00:53<00:00,  1.06it/s, train_mse_loss_step=0.156]Epoch 0: |          | 56/? [00:53<00:00,  1.06it/s, train_mse_loss_step=0.154]Epoch 0: |          | 57/? [00:53<00:00,  1.06it/s, train_mse_loss_step=0.154]Epoch 0: |          | 57/? [00:53<00:00,  1.06it/s, train_mse_loss_step=0.162]Epoch 0: |          | 58/? [00:54<00:00,  1.07it/s, train_mse_loss_step=0.162]Epoch 0: |          | 58/? [00:54<00:00,  1.07it/s, train_mse_loss_step=0.154]Epoch 0: |          | 59/? [00:54<00:00,  1.08it/s, train_mse_loss_step=0.154]Epoch 0: |          | 59/? [00:54<00:00,  1.08it/s, train_mse_loss_step=0.150]Epoch 0: |          | 60/? [00:55<00:00,  1.08it/s, train_mse_loss_step=0.150]Epoch 0: |          | 60/? [00:55<00:00,  1.08it/s, train_mse_loss_step=0.155]Epoch 0: |          | 61/? [00:55<00:00,  1.09it/s, train_mse_loss_step=0.155]Epoch 0: |          | 61/? [00:55<00:00,  1.09it/s, train_mse_loss_step=0.139]Epoch 0: |          | 62/? [00:56<00:00,  1.10it/s, train_mse_loss_step=0.139]Epoch 0: |          | 62/? [00:56<00:00,  1.10it/s, train_mse_loss_step=0.142]Epoch 0: |          | 63/? [00:57<00:00,  1.10it/s, train_mse_loss_step=0.142]Epoch 0: |          | 63/? [00:57<00:00,  1.10it/s, train_mse_loss_step=0.141]Epoch 0: |          | 64/? [00:57<00:00,  1.11it/s, train_mse_loss_step=0.141]Epoch 0: |          | 64/? [00:57<00:00,  1.11it/s, train_mse_loss_step=0.136]Epoch 0: |          | 65/? [00:58<00:00,  1.12it/s, train_mse_loss_step=0.136]Epoch 0: |          | 65/? [00:58<00:00,  1.12it/s, train_mse_loss_step=0.135]Epoch 0: |          | 66/? [00:58<00:00,  1.12it/s, train_mse_loss_step=0.135]Epoch 0: |          | 66/? [00:58<00:00,  1.12it/s, train_mse_loss_step=0.126]Epoch 0: |          | 67/? [00:59<00:00,  1.13it/s, train_mse_loss_step=0.126]Epoch 0: |          | 67/? [00:59<00:00,  1.13it/s, train_mse_loss_step=0.128]Epoch 0: |          | 68/? [00:59<00:00,  1.14it/s, train_mse_loss_step=0.128]Epoch 0: |          | 68/? [00:59<00:00,  1.14it/s, train_mse_loss_step=0.123]Epoch 0: |          | 69/? [01:00<00:00,  1.14it/s, train_mse_loss_step=0.123]Epoch 0: |          | 69/? [01:00<00:00,  1.14it/s, train_mse_loss_step=0.129]Epoch 0: |          | 70/? [01:01<00:00,  1.15it/s, train_mse_loss_step=0.129]Epoch 0: |          | 70/? [01:01<00:00,  1.15it/s, train_mse_loss_step=0.126]Epoch 0: |          | 71/? [01:01<00:00,  1.15it/s, train_mse_loss_step=0.126]Epoch 0: |          | 71/? [01:01<00:00,  1.15it/s, train_mse_loss_step=0.127]Epoch 0: |          | 72/? [01:02<00:00,  1.16it/s, train_mse_loss_step=0.127]Epoch 0: |          | 72/? [01:02<00:00,  1.16it/s, train_mse_loss_step=0.118]Epoch 0: |          | 73/? [01:02<00:00,  1.16it/s, train_mse_loss_step=0.118]Epoch 0: |          | 73/? [01:02<00:00,  1.16it/s, train_mse_loss_step=0.119]Epoch 0: |          | 74/? [01:03<00:00,  1.17it/s, train_mse_loss_step=0.119]Epoch 0: |          | 74/? [01:03<00:00,  1.17it/s, train_mse_loss_step=0.123]Epoch 0: |          | 75/? [01:03<00:00,  1.17it/s, train_mse_loss_step=0.123]Epoch 0: |          | 75/? [01:03<00:00,  1.17it/s, train_mse_loss_step=0.109]Epoch 0: |          | 76/? [01:04<00:00,  1.18it/s, train_mse_loss_step=0.109]Epoch 0: |          | 76/? [01:04<00:00,  1.18it/s, train_mse_loss_step=0.118]Epoch 0: |          | 77/? [01:05<00:00,  1.18it/s, train_mse_loss_step=0.118]Epoch 0: |          | 77/? [01:05<00:00,  1.18it/s, train_mse_loss_step=0.110]Epoch 0: |          | 78/? [01:05<00:00,  1.19it/s, train_mse_loss_step=0.110]Epoch 0: |          | 78/? [01:05<00:00,  1.19it/s, train_mse_loss_step=0.114]Epoch 0: |          | 79/? [01:06<00:00,  1.19it/s, train_mse_loss_step=0.114]Epoch 0: |          | 79/? [01:06<00:00,  1.19it/s, train_mse_loss_step=0.108]Epoch 0: |          | 80/? [01:06<00:00,  1.20it/s, train_mse_loss_step=0.108]Epoch 0: |          | 80/? [01:06<00:00,  1.20it/s, train_mse_loss_step=0.111]Epoch 0: |          | 81/? [01:07<00:00,  1.20it/s, train_mse_loss_step=0.111]Epoch 0: |          | 81/? [01:07<00:00,  1.20it/s, train_mse_loss_step=0.101]Epoch 0: |          | 82/? [01:07<00:00,  1.21it/s, train_mse_loss_step=0.101]Epoch 0: |          | 82/? [01:07<00:00,  1.21it/s, train_mse_loss_step=0.110]Epoch 0: |          | 83/? [01:08<00:00,  1.21it/s, train_mse_loss_step=0.110]Epoch 0: |          | 83/? [01:08<00:00,  1.21it/s, train_mse_loss_step=0.111]Epoch 0: |          | 84/? [01:09<00:00,  1.22it/s, train_mse_loss_step=0.111]Epoch 0: |          | 84/? [01:09<00:00,  1.22it/s, train_mse_loss_step=0.104]Epoch 0: |          | 85/? [01:09<00:00,  1.22it/s, train_mse_loss_step=0.104]Epoch 0: |          | 85/? [01:09<00:00,  1.22it/s, train_mse_loss_step=0.103]Epoch 0: |          | 86/? [01:10<00:00,  1.23it/s, train_mse_loss_step=0.103]Epoch 0: |          | 86/? [01:10<00:00,  1.23it/s, train_mse_loss_step=0.0946]Epoch 0: |          | 87/? [01:10<00:00,  1.23it/s, train_mse_loss_step=0.0946]Epoch 0: |          | 87/? [01:10<00:00,  1.23it/s, train_mse_loss_step=0.102] Epoch 0: |          | 88/? [01:11<00:00,  1.23it/s, train_mse_loss_step=0.102]Epoch 0: |          | 88/? [01:11<00:00,  1.23it/s, train_mse_loss_step=0.0991]Epoch 0: |          | 89/? [01:11<00:00,  1.24it/s, train_mse_loss_step=0.0991]Epoch 0: |          | 89/? [01:11<00:00,  1.24it/s, train_mse_loss_step=0.0943]Epoch 0: |          | 90/? [01:12<00:00,  1.24it/s, train_mse_loss_step=0.0943]Epoch 0: |          | 90/? [01:12<00:00,  1.24it/s, train_mse_loss_step=0.0972]Epoch 0: |          | 91/? [01:13<00:00,  1.25it/s, train_mse_loss_step=0.0972]Epoch 0: |          | 91/? [01:13<00:00,  1.25it/s, train_mse_loss_step=0.101] Epoch 0: |          | 92/? [01:13<00:00,  1.25it/s, train_mse_loss_step=0.101]Epoch 0: |          | 92/? [01:13<00:00,  1.25it/s, train_mse_loss_step=0.0999]Epoch 0: |          | 93/? [01:14<00:00,  1.25it/s, train_mse_loss_step=0.0999]Epoch 0: |          | 93/? [01:14<00:00,  1.25it/s, train_mse_loss_step=0.0894]Epoch 0: |          | 94/? [01:14<00:00,  1.26it/s, train_mse_loss_step=0.0894]Epoch 0: |          | 94/? [01:14<00:00,  1.26it/s, train_mse_loss_step=0.0877]Epoch 0: |          | 95/? [01:15<00:00,  1.26it/s, train_mse_loss_step=0.0877]Epoch 0: |          | 95/? [01:15<00:00,  1.26it/s, train_mse_loss_step=0.0914]Epoch 0: |          | 96/? [01:15<00:00,  1.26it/s, train_mse_loss_step=0.0914]Epoch 0: |          | 96/? [01:15<00:00,  1.26it/s, train_mse_loss_step=0.0923]Epoch 0: |          | 97/? [01:16<00:00,  1.27it/s, train_mse_loss_step=0.0923]Epoch 0: |          | 97/? [01:16<00:00,  1.27it/s, train_mse_loss_step=0.0933]Epoch 0: |          | 98/? [01:17<00:00,  1.27it/s, train_mse_loss_step=0.0933]Epoch 0: |          | 98/? [01:17<00:00,  1.27it/s, train_mse_loss_step=0.0919]Epoch 0: |          | 99/? [01:17<00:00,  1.28it/s, train_mse_loss_step=0.0919]Epoch 0: |          | 99/? [01:17<00:00,  1.28it/s, train_mse_loss_step=0.0847]Epoch 0: |          | 100/? [01:18<00:00,  1.28it/s, train_mse_loss_step=0.0847]Epoch 0: |          | 100/? [01:18<00:00,  1.28it/s, train_mse_loss_step=0.0843]Epoch 0: |          | 101/? [01:18<00:00,  1.28it/s, train_mse_loss_step=0.0843]Epoch 0: |          | 101/? [01:18<00:00,  1.28it/s, train_mse_loss_step=0.0816]Epoch 0: |          | 102/? [01:19<00:00,  1.29it/s, train_mse_loss_step=0.0816]Epoch 0: |          | 102/? [01:19<00:00,  1.29it/s, train_mse_loss_step=0.0837]Epoch 0: |          | 103/? [01:19<00:00,  1.29it/s, train_mse_loss_step=0.0837]Epoch 0: |          | 103/? [01:19<00:00,  1.29it/s, train_mse_loss_step=0.0823]Epoch 0: |          | 104/? [01:20<00:00,  1.29it/s, train_mse_loss_step=0.0823]Epoch 0: |          | 104/? [01:20<00:00,  1.29it/s, train_mse_loss_step=0.083] Epoch 0: |          | 105/? [01:21<00:00,  1.30it/s, train_mse_loss_step=0.083]Epoch 0: |          | 105/? [01:21<00:00,  1.30it/s, train_mse_loss_step=0.0867]Epoch 0: |          | 106/? [01:21<00:00,  1.30it/s, train_mse_loss_step=0.0867]Epoch 0: |          | 106/? [01:21<00:00,  1.30it/s, train_mse_loss_step=0.0786]Epoch 0: |          | 107/? [01:22<00:00,  1.30it/s, train_mse_loss_step=0.0786]Epoch 0: |          | 107/? [01:22<00:00,  1.30it/s, train_mse_loss_step=0.0805]Epoch 0: |          | 108/? [01:22<00:00,  1.31it/s, train_mse_loss_step=0.0805]Epoch 0: |          | 108/? [01:22<00:00,  1.31it/s, train_mse_loss_step=0.0782]Epoch 0: |          | 109/? [01:23<00:00,  1.31it/s, train_mse_loss_step=0.0782]Epoch 0: |          | 109/? [01:23<00:00,  1.31it/s, train_mse_loss_step=0.0743]Epoch 0: |          | 110/? [01:23<00:00,  1.31it/s, train_mse_loss_step=0.0743]Epoch 0: |          | 110/? [01:23<00:00,  1.31it/s, train_mse_loss_step=0.0756]Epoch 0: |          | 111/? [01:24<00:00,  1.31it/s, train_mse_loss_step=0.0756]Epoch 0: |          | 111/? [01:24<00:00,  1.31it/s, train_mse_loss_step=0.0779]Epoch 0: |          | 112/? [01:25<00:00,  1.32it/s, train_mse_loss_step=0.0779]Epoch 0: |          | 112/? [01:25<00:00,  1.32it/s, train_mse_loss_step=0.0687]Epoch 0: |          | 113/? [01:25<00:00,  1.32it/s, train_mse_loss_step=0.0687]Epoch 0: |          | 113/? [01:25<00:00,  1.32it/s, train_mse_loss_step=0.0805]Epoch 0: |          | 114/? [01:26<00:00,  1.32it/s, train_mse_loss_step=0.0805]Epoch 0: |          | 114/? [01:26<00:00,  1.32it/s, train_mse_loss_step=0.0698]Epoch 0: |          | 115/? [01:26<00:00,  1.33it/s, train_mse_loss_step=0.0698]Epoch 0: |          | 115/? [01:26<00:00,  1.33it/s, train_mse_loss_step=0.0806]Epoch 0: |          | 116/? [01:27<00:00,  1.33it/s, train_mse_loss_step=0.0806]Epoch 0: |          | 116/? [01:27<00:00,  1.33it/s, train_mse_loss_step=0.0736]Epoch 0: |          | 117/? [01:27<00:00,  1.33it/s, train_mse_loss_step=0.0736]Epoch 0: |          | 117/? [01:27<00:00,  1.33it/s, train_mse_loss_step=0.0711]Epoch 0: |          | 118/? [01:28<00:00,  1.33it/s, train_mse_loss_step=0.0711]Epoch 0: |          | 118/? [01:28<00:00,  1.33it/s, train_mse_loss_step=0.0689]Epoch 0: |          | 119/? [01:29<00:00,  1.34it/s, train_mse_loss_step=0.0689]Epoch 0: |          | 119/? [01:29<00:00,  1.34it/s, train_mse_loss_step=0.0698]Epoch 0: |          | 120/? [01:29<00:00,  1.34it/s, train_mse_loss_step=0.0698]Epoch 0: |          | 120/? [01:29<00:00,  1.34it/s, train_mse_loss_step=0.0769]Epoch 0: |          | 121/? [01:30<00:00,  1.34it/s, train_mse_loss_step=0.0769]Epoch 0: |          | 121/? [01:30<00:00,  1.34it/s, train_mse_loss_step=0.0699]Epoch 0: |          | 122/? [01:30<00:00,  1.34it/s, train_mse_loss_step=0.0699]Epoch 0: |          | 122/? [01:30<00:00,  1.34it/s, train_mse_loss_step=0.0684]Epoch 0: |          | 123/? [01:31<00:00,  1.35it/s, train_mse_loss_step=0.0684]Epoch 0: |          | 123/? [01:31<00:00,  1.35it/s, train_mse_loss_step=0.0729]Epoch 0: |          | 124/? [01:31<00:00,  1.35it/s, train_mse_loss_step=0.0729]Epoch 0: |          | 124/? [01:31<00:00,  1.35it/s, train_mse_loss_step=0.0668]Epoch 0: |          | 125/? [01:32<00:00,  1.35it/s, train_mse_loss_step=0.0668]Epoch 0: |          | 125/? [01:32<00:00,  1.35it/s, train_mse_loss_step=0.0652]Epoch 0: |          | 126/? [01:33<00:00,  1.35it/s, train_mse_loss_step=0.0652]Epoch 0: |          | 126/? [01:33<00:00,  1.35it/s, train_mse_loss_step=0.0675]Epoch 0: |          | 127/? [01:33<00:00,  1.36it/s, train_mse_loss_step=0.0675]Epoch 0: |          | 127/? [01:33<00:00,  1.36it/s, train_mse_loss_step=0.0687]Epoch 0: |          | 128/? [01:34<00:00,  1.36it/s, train_mse_loss_step=0.0687]Epoch 0: |          | 128/? [01:34<00:00,  1.36it/s, train_mse_loss_step=0.0658]Epoch 0: |          | 129/? [01:34<00:00,  1.36it/s, train_mse_loss_step=0.0658]Epoch 0: |          | 129/? [01:34<00:00,  1.36it/s, train_mse_loss_step=0.0641]Epoch 0: |          | 130/? [01:35<00:00,  1.36it/s, train_mse_loss_step=0.0641]Epoch 0: |          | 130/? [01:35<00:00,  1.36it/s, train_mse_loss_step=0.0591]Epoch 0: |          | 131/? [01:36<00:00,  1.36it/s, train_mse_loss_step=0.0591]Epoch 0: |          | 131/? [01:36<00:00,  1.36it/s, train_mse_loss_step=0.0696]Epoch 0: |          | 132/? [01:36<00:00,  1.37it/s, train_mse_loss_step=0.0696]Epoch 0: |          | 132/? [01:36<00:00,  1.37it/s, train_mse_loss_step=0.0668]Epoch 0: |          | 133/? [01:37<00:00,  1.37it/s, train_mse_loss_step=0.0668]Epoch 0: |          | 133/? [01:37<00:00,  1.37it/s, train_mse_loss_step=0.0629]Epoch 0: |          | 134/? [01:37<00:00,  1.37it/s, train_mse_loss_step=0.0629]Epoch 0: |          | 134/? [01:37<00:00,  1.37it/s, train_mse_loss_step=0.0603]Epoch 0: |          | 135/? [01:38<00:00,  1.37it/s, train_mse_loss_step=0.0603]Epoch 0: |          | 135/? [01:38<00:00,  1.37it/s, train_mse_loss_step=0.068] Epoch 0: |          | 136/? [01:38<00:00,  1.38it/s, train_mse_loss_step=0.068]Epoch 0: |          | 136/? [01:38<00:00,  1.38it/s, train_mse_loss_step=0.0623]Epoch 0: |          | 137/? [01:39<00:00,  1.38it/s, train_mse_loss_step=0.0623]Epoch 0: |          | 137/? [01:39<00:00,  1.38it/s, train_mse_loss_step=0.0653]Epoch 0: |          | 138/? [01:40<00:00,  1.38it/s, train_mse_loss_step=0.0653]Epoch 0: |          | 138/? [01:40<00:00,  1.38it/s, train_mse_loss_step=0.0623]Epoch 0: |          | 139/? [01:41<00:00,  1.37it/s, train_mse_loss_step=0.0623]Epoch 0: |          | 139/? [01:41<00:00,  1.37it/s, train_mse_loss_step=0.0659]Epoch 0: |          | 140/? [01:41<00:00,  1.37it/s, train_mse_loss_step=0.0659]Epoch 0: |          | 140/? [01:41<00:00,  1.37it/s, train_mse_loss_step=0.0638]Epoch 0: |          | 141/? [01:42<00:00,  1.38it/s, train_mse_loss_step=0.0638]Epoch 0: |          | 141/? [01:42<00:00,  1.38it/s, train_mse_loss_step=0.0654]Epoch 0: |          | 142/? [01:42<00:00,  1.38it/s, train_mse_loss_step=0.0654]Epoch 0: |          | 142/? [01:42<00:00,  1.38it/s, train_mse_loss_step=0.0579]Epoch 0: |          | 143/? [01:43<00:00,  1.38it/s, train_mse_loss_step=0.0579]Epoch 0: |          | 143/? [01:43<00:00,  1.38it/s, train_mse_loss_step=0.0588]Epoch 0: |          | 144/? [01:44<00:00,  1.38it/s, train_mse_loss_step=0.0588]Epoch 0: |          | 144/? [01:44<00:00,  1.38it/s, train_mse_loss_step=0.0555]Epoch 0: |          | 145/? [01:44<00:00,  1.39it/s, train_mse_loss_step=0.0555]Epoch 0: |          | 145/? [01:44<00:00,  1.38it/s, train_mse_loss_step=0.0615]Epoch 0: |          | 146/? [01:45<00:00,  1.39it/s, train_mse_loss_step=0.0615]Epoch 0: |          | 146/? [01:45<00:00,  1.39it/s, train_mse_loss_step=0.0621]Epoch 0: |          | 147/? [01:45<00:00,  1.39it/s, train_mse_loss_step=0.0621]Epoch 0: |          | 147/? [01:45<00:00,  1.39it/s, train_mse_loss_step=0.0638]Epoch 0: |          | 148/? [01:46<00:00,  1.39it/s, train_mse_loss_step=0.0638]Epoch 0: |          | 148/? [01:46<00:00,  1.39it/s, train_mse_loss_step=0.0632]Epoch 0: |          | 149/? [01:46<00:00,  1.39it/s, train_mse_loss_step=0.0632]Epoch 0: |          | 149/? [01:46<00:00,  1.39it/s, train_mse_loss_step=0.0552]Epoch 0: |          | 150/? [01:47<00:00,  1.39it/s, train_mse_loss_step=0.0552]Epoch 0: |          | 150/? [01:47<00:00,  1.39it/s, train_mse_loss_step=0.0545]Epoch 0: |          | 151/? [01:48<00:00,  1.40it/s, train_mse_loss_step=0.0545]Epoch 0: |          | 151/? [01:48<00:00,  1.40it/s, train_mse_loss_step=0.0576]Epoch 0: |          | 152/? [01:48<00:00,  1.40it/s, train_mse_loss_step=0.0576]Epoch 0: |          | 152/? [01:48<00:00,  1.40it/s, train_mse_loss_step=0.0586]Epoch 0: |          | 153/? [01:49<00:00,  1.40it/s, train_mse_loss_step=0.0586]Epoch 0: |          | 153/? [01:49<00:00,  1.40it/s, train_mse_loss_step=0.0552]Epoch 0: |          | 154/? [01:49<00:00,  1.40it/s, train_mse_loss_step=0.0552]Epoch 0: |          | 154/? [01:49<00:00,  1.40it/s, train_mse_loss_step=0.063] Epoch 0: |          | 155/? [01:50<00:00,  1.40it/s, train_mse_loss_step=0.063]Epoch 0: |          | 155/? [01:50<00:00,  1.40it/s, train_mse_loss_step=0.0598]Epoch 0: |          | 156/? [01:50<00:00,  1.41it/s, train_mse_loss_step=0.0598]Epoch 0: |          | 156/? [01:50<00:00,  1.41it/s, train_mse_loss_step=0.0638]Epoch 0: |          | 157/? [01:51<00:00,  1.41it/s, train_mse_loss_step=0.0638]Epoch 0: |          | 157/? [01:51<00:00,  1.41it/s, train_mse_loss_step=0.0564]Epoch 0: |          | 158/? [01:52<00:00,  1.41it/s, train_mse_loss_step=0.0564]Epoch 0: |          | 158/? [01:52<00:00,  1.41it/s, train_mse_loss_step=0.0605]Epoch 0: |          | 159/? [01:52<00:00,  1.41it/s, train_mse_loss_step=0.0605]Epoch 0: |          | 159/? [01:52<00:00,  1.41it/s, train_mse_loss_step=0.0548]Epoch 0: |          | 160/? [01:53<00:00,  1.41it/s, train_mse_loss_step=0.0548]Epoch 0: |          | 160/? [01:53<00:00,  1.41it/s, train_mse_loss_step=0.055] Epoch 0: |          | 161/? [01:53<00:00,  1.41it/s, train_mse_loss_step=0.055]Epoch 0: |          | 161/? [01:53<00:00,  1.41it/s, train_mse_loss_step=0.0586]Epoch 0: |          | 162/? [01:54<00:00,  1.42it/s, train_mse_loss_step=0.0586]Epoch 0: |          | 162/? [01:54<00:00,  1.42it/s, train_mse_loss_step=0.0564]Epoch 0: |          | 163/? [01:54<00:00,  1.42it/s, train_mse_loss_step=0.0564]Epoch 0: |          | 163/? [01:54<00:00,  1.42it/s, train_mse_loss_step=0.0541]Epoch 0: |          | 164/? [01:55<00:00,  1.42it/s, train_mse_loss_step=0.0541]Epoch 0: |          | 164/? [01:55<00:00,  1.42it/s, train_mse_loss_step=0.0577]Epoch 0: |          | 165/? [01:56<00:00,  1.42it/s, train_mse_loss_step=0.0577]Epoch 0: |          | 165/? [01:56<00:00,  1.42it/s, train_mse_loss_step=0.0539]Epoch 0: |          | 166/? [01:56<00:00,  1.42it/s, train_mse_loss_step=0.0539]Epoch 0: |          | 166/? [01:56<00:00,  1.42it/s, train_mse_loss_step=0.0536]Epoch 0: |          | 167/? [01:57<00:00,  1.42it/s, train_mse_loss_step=0.0536]Epoch 0: |          | 167/? [01:57<00:00,  1.42it/s, train_mse_loss_step=0.051] Epoch 0: |          | 168/? [01:57<00:00,  1.43it/s, train_mse_loss_step=0.051]Epoch 0: |          | 168/? [01:57<00:00,  1.43it/s, train_mse_loss_step=0.055]Epoch 0: |          | 169/? [01:58<00:00,  1.43it/s, train_mse_loss_step=0.055]Epoch 0: |          | 169/? [01:58<00:00,  1.43it/s, train_mse_loss_step=0.0553]Epoch 0: |          | 170/? [01:59<00:00,  1.43it/s, train_mse_loss_step=0.0553]Epoch 0: |          | 170/? [01:59<00:00,  1.43it/s, train_mse_loss_step=0.0545]Epoch 0: |          | 171/? [01:59<00:00,  1.43it/s, train_mse_loss_step=0.0545]Epoch 0: |          | 171/? [01:59<00:00,  1.43it/s, train_mse_loss_step=0.0491]Epoch 0: |          | 172/? [02:00<00:00,  1.43it/s, train_mse_loss_step=0.0491]Epoch 0: |          | 172/? [02:00<00:00,  1.43it/s, train_mse_loss_step=0.0566]Epoch 0: |          | 173/? [02:00<00:00,  1.43it/s, train_mse_loss_step=0.0566]Epoch 0: |          | 173/? [02:00<00:00,  1.43it/s, train_mse_loss_step=0.0563]Epoch 0: |          | 174/? [02:01<00:00,  1.43it/s, train_mse_loss_step=0.0563]Epoch 0: |          | 174/? [02:01<00:00,  1.43it/s, train_mse_loss_step=0.0535]Epoch 0: |          | 175/? [02:01<00:00,  1.44it/s, train_mse_loss_step=0.0535]Epoch 0: |          | 175/? [02:01<00:00,  1.44it/s, train_mse_loss_step=0.0546]Epoch 0: |          | 176/? [02:02<00:00,  1.44it/s, train_mse_loss_step=0.0546]Epoch 0: |          | 176/? [02:02<00:00,  1.44it/s, train_mse_loss_step=0.0545]Epoch 0: |          | 177/? [02:03<00:00,  1.44it/s, train_mse_loss_step=0.0545]Epoch 0: |          | 177/? [02:03<00:00,  1.44it/s, train_mse_loss_step=0.0505]Epoch 0: |          | 178/? [02:03<00:00,  1.44it/s, train_mse_loss_step=0.0505]Epoch 0: |          | 178/? [02:03<00:00,  1.44it/s, train_mse_loss_step=0.0489]Epoch 0: |          | 179/? [02:04<00:00,  1.44it/s, train_mse_loss_step=0.0489]Epoch 0: |          | 179/? [02:04<00:00,  1.44it/s, train_mse_loss_step=0.0586]Epoch 0: |          | 180/? [02:04<00:00,  1.44it/s, train_mse_loss_step=0.0586]Epoch 0: |          | 180/? [02:04<00:00,  1.44it/s, train_mse_loss_step=0.051] Epoch 0: |          | 181/? [02:05<00:00,  1.44it/s, train_mse_loss_step=0.051]Epoch 0: |          | 181/? [02:05<00:00,  1.44it/s, train_mse_loss_step=0.0564]Epoch 0: |          | 182/? [02:05<00:00,  1.45it/s, train_mse_loss_step=0.0564]Epoch 0: |          | 182/? [02:05<00:00,  1.45it/s, train_mse_loss_step=0.0554]Epoch 0: |          | 183/? [02:06<00:00,  1.45it/s, train_mse_loss_step=0.0554]Epoch 0: |          | 183/? [02:06<00:00,  1.45it/s, train_mse_loss_step=0.0488]Epoch 0: |          | 184/? [02:07<00:00,  1.45it/s, train_mse_loss_step=0.0488]Epoch 0: |          | 184/? [02:07<00:00,  1.45it/s, train_mse_loss_step=0.0526]Epoch 0: |          | 185/? [02:07<00:00,  1.45it/s, train_mse_loss_step=0.0526]Epoch 0: |          | 185/? [02:07<00:00,  1.45it/s, train_mse_loss_step=0.0532]Epoch 0: |          | 186/? [02:08<00:00,  1.45it/s, train_mse_loss_step=0.0532]Epoch 0: |          | 186/? [02:08<00:00,  1.45it/s, train_mse_loss_step=0.0481]Epoch 0: |          | 187/? [02:08<00:00,  1.45it/s, train_mse_loss_step=0.0481]Epoch 0: |          | 187/? [02:08<00:00,  1.45it/s, train_mse_loss_step=0.0507]Epoch 0: |          | 188/? [02:09<00:00,  1.45it/s, train_mse_loss_step=0.0507]Epoch 0: |          | 188/? [02:09<00:00,  1.45it/s, train_mse_loss_step=0.0505]Epoch 0: |          | 189/? [02:09<00:00,  1.45it/s, train_mse_loss_step=0.0505]Epoch 0: |          | 189/? [02:09<00:00,  1.45it/s, train_mse_loss_step=0.0523]Epoch 0: |          | 190/? [02:10<00:00,  1.46it/s, train_mse_loss_step=0.0523]Epoch 0: |          | 190/? [02:10<00:00,  1.46it/s, train_mse_loss_step=0.0458]Epoch 0: |          | 191/? [02:11<00:00,  1.46it/s, train_mse_loss_step=0.0458]Epoch 0: |          | 191/? [02:11<00:00,  1.46it/s, train_mse_loss_step=0.0487]Epoch 0: |          | 192/? [02:11<00:00,  1.46it/s, train_mse_loss_step=0.0487]Epoch 0: |          | 192/? [02:11<00:00,  1.46it/s, train_mse_loss_step=0.0562]Epoch 0: |          | 193/? [02:12<00:00,  1.46it/s, train_mse_loss_step=0.0562]Epoch 0: |          | 193/? [02:12<00:00,  1.46it/s, train_mse_loss_step=0.0511]Epoch 0: |          | 194/? [02:12<00:00,  1.46it/s, train_mse_loss_step=0.0511]Epoch 0: |          | 194/? [02:12<00:00,  1.46it/s, train_mse_loss_step=0.0589]Epoch 0: |          | 195/? [02:13<00:00,  1.46it/s, train_mse_loss_step=0.0589]Epoch 0: |          | 195/? [02:13<00:00,  1.46it/s, train_mse_loss_step=0.0516]Epoch 0: |          | 196/? [02:13<00:00,  1.46it/s, train_mse_loss_step=0.0516]Epoch 0: |          | 196/? [02:13<00:00,  1.46it/s, train_mse_loss_step=0.0519]Epoch 0: |          | 197/? [02:14<00:00,  1.47it/s, train_mse_loss_step=0.0519]Epoch 0: |          | 197/? [02:14<00:00,  1.46it/s, train_mse_loss_step=0.0505]Epoch 0: |          | 198/? [02:15<00:00,  1.47it/s, train_mse_loss_step=0.0505]Epoch 0: |          | 198/? [02:15<00:00,  1.47it/s, train_mse_loss_step=0.0503]Epoch 0: |          | 199/? [02:15<00:00,  1.47it/s, train_mse_loss_step=0.0503]Epoch 0: |          | 199/? [02:15<00:00,  1.47it/s, train_mse_loss_step=0.0479]Epoch 0: |          | 200/? [02:16<00:00,  1.47it/s, train_mse_loss_step=0.0479]Epoch 0: |          | 200/? [02:16<00:00,  1.47it/s, train_mse_loss_step=0.0524]Epoch 0: |          | 201/? [02:16<00:00,  1.47it/s, train_mse_loss_step=0.0524]Epoch 0: |          | 201/? [02:16<00:00,  1.47it/s, train_mse_loss_step=0.052] Epoch 0: |          | 202/? [02:17<00:00,  1.47it/s, train_mse_loss_step=0.052]Epoch 0: |          | 202/? [02:17<00:00,  1.47it/s, train_mse_loss_step=0.051]Epoch 0: |          | 203/? [02:17<00:00,  1.47it/s, train_mse_loss_step=0.051]Epoch 0: |          | 203/? [02:17<00:00,  1.47it/s, train_mse_loss_step=0.0538]Epoch 0: |          | 204/? [02:18<00:00,  1.47it/s, train_mse_loss_step=0.0538]Epoch 0: |          | 204/? [02:18<00:00,  1.47it/s, train_mse_loss_step=0.0471]Epoch 0: |          | 205/? [02:19<00:00,  1.47it/s, train_mse_loss_step=0.0471]Epoch 0: |          | 205/? [02:19<00:00,  1.47it/s, train_mse_loss_step=0.0474]Epoch 0: |          | 206/? [02:19<00:00,  1.48it/s, train_mse_loss_step=0.0474]Epoch 0: |          | 206/? [02:19<00:00,  1.47it/s, train_mse_loss_step=0.0488]Epoch 0: |          | 207/? [02:20<00:00,  1.48it/s, train_mse_loss_step=0.0488]Epoch 0: |          | 207/? [02:20<00:00,  1.48it/s, train_mse_loss_step=0.0467]Epoch 0: |          | 208/? [02:20<00:00,  1.48it/s, train_mse_loss_step=0.0467]Epoch 0: |          | 208/? [02:20<00:00,  1.48it/s, train_mse_loss_step=0.0555]Epoch 0: |          | 209/? [02:21<00:00,  1.48it/s, train_mse_loss_step=0.0555]Epoch 0: |          | 209/? [02:21<00:00,  1.48it/s, train_mse_loss_step=0.0454]Epoch 0: |          | 210/? [02:21<00:00,  1.48it/s, train_mse_loss_step=0.0454]Epoch 0: |          | 210/? [02:21<00:00,  1.48it/s, train_mse_loss_step=0.0507]Epoch 0: |          | 211/? [02:22<00:00,  1.48it/s, train_mse_loss_step=0.0507]Epoch 0: |          | 211/? [02:22<00:00,  1.48it/s, train_mse_loss_step=0.0481]Epoch 0: |          | 212/? [02:23<00:00,  1.48it/s, train_mse_loss_step=0.0481]Epoch 0: |          | 212/? [02:23<00:00,  1.48it/s, train_mse_loss_step=0.0511]Epoch 0: |          | 213/? [02:23<00:00,  1.48it/s, train_mse_loss_step=0.0511]Epoch 0: |          | 213/? [02:23<00:00,  1.48it/s, train_mse_loss_step=0.0507]Epoch 0: |          | 214/? [02:24<00:00,  1.48it/s, train_mse_loss_step=0.0507]Epoch 0: |          | 214/? [02:24<00:00,  1.48it/s, train_mse_loss_step=0.0478]Epoch 0: |          | 215/? [02:24<00:00,  1.48it/s, train_mse_loss_step=0.0478]Epoch 0: |          | 215/? [02:24<00:00,  1.48it/s, train_mse_loss_step=0.0472]Epoch 0: |          | 216/? [02:25<00:00,  1.48it/s, train_mse_loss_step=0.0472]Epoch 0: |          | 216/? [02:25<00:00,  1.48it/s, train_mse_loss_step=0.046] Epoch 0: |          | 217/? [02:26<00:00,  1.48it/s, train_mse_loss_step=0.046]Epoch 0: |          | 217/? [02:26<00:00,  1.48it/s, train_mse_loss_step=0.0543]Epoch 0: |          | 218/? [02:26<00:00,  1.49it/s, train_mse_loss_step=0.0543]Epoch 0: |          | 218/? [02:26<00:00,  1.48it/s, train_mse_loss_step=0.0441]Epoch 0: |          | 219/? [02:27<00:00,  1.49it/s, train_mse_loss_step=0.0441]Epoch 0: |          | 219/? [02:27<00:00,  1.49it/s, train_mse_loss_step=0.0471]Epoch 0: |          | 220/? [02:28<00:00,  1.49it/s, train_mse_loss_step=0.0471]Epoch 0: |          | 220/? [02:28<00:00,  1.49it/s, train_mse_loss_step=0.0425]Epoch 0: |          | 221/? [02:28<00:00,  1.49it/s, train_mse_loss_step=0.0425]Epoch 0: |          | 221/? [02:28<00:00,  1.49it/s, train_mse_loss_step=0.0496]Epoch 0: |          | 222/? [02:29<00:00,  1.49it/s, train_mse_loss_step=0.0496]Epoch 0: |          | 222/? [02:29<00:00,  1.49it/s, train_mse_loss_step=0.0483]Epoch 0: |          | 223/? [02:29<00:00,  1.49it/s, train_mse_loss_step=0.0483]Epoch 0: |          | 223/? [02:29<00:00,  1.49it/s, train_mse_loss_step=0.0481]Epoch 0: |          | 224/? [02:30<00:00,  1.49it/s, train_mse_loss_step=0.0481]Epoch 0: |          | 224/? [02:30<00:00,  1.49it/s, train_mse_loss_step=0.0456]Epoch 0: |          | 225/? [02:30<00:00,  1.49it/s, train_mse_loss_step=0.0456]Epoch 0: |          | 225/? [02:30<00:00,  1.49it/s, train_mse_loss_step=0.0461]Epoch 0: |          | 226/? [02:31<00:00,  1.49it/s, train_mse_loss_step=0.0461]Epoch 0: |          | 226/? [02:31<00:00,  1.49it/s, train_mse_loss_step=0.0431]Epoch 0: |          | 227/? [02:32<00:00,  1.49it/s, train_mse_loss_step=0.0431]Epoch 0: |          | 227/? [02:32<00:00,  1.49it/s, train_mse_loss_step=0.0526]Epoch 0: |          | 228/? [02:32<00:00,  1.49it/s, train_mse_loss_step=0.0526]Epoch 0: |          | 228/? [02:32<00:00,  1.49it/s, train_mse_loss_step=0.0498]Epoch 0: |          | 229/? [02:33<00:00,  1.49it/s, train_mse_loss_step=0.0498]Epoch 0: |          | 229/? [02:33<00:00,  1.49it/s, train_mse_loss_step=0.0473]Epoch 0: |          | 230/? [02:33<00:00,  1.50it/s, train_mse_loss_step=0.0473]Epoch 0: |          | 230/? [02:33<00:00,  1.50it/s, train_mse_loss_step=0.0452]Epoch 0: |          | 231/? [02:34<00:00,  1.50it/s, train_mse_loss_step=0.0452]Epoch 0: |          | 231/? [02:34<00:00,  1.50it/s, train_mse_loss_step=0.0494]Epoch 0: |          | 232/? [02:34<00:00,  1.50it/s, train_mse_loss_step=0.0494]Epoch 0: |          | 232/? [02:34<00:00,  1.50it/s, train_mse_loss_step=0.0531]Epoch 0: |          | 233/? [02:35<00:00,  1.50it/s, train_mse_loss_step=0.0531]Epoch 0: |          | 233/? [02:35<00:00,  1.50it/s, train_mse_loss_step=0.0511]Epoch 0: |          | 234/? [02:36<00:00,  1.50it/s, train_mse_loss_step=0.0511]Epoch 0: |          | 234/? [02:36<00:00,  1.50it/s, train_mse_loss_step=0.0457]Epoch 0: |          | 235/? [02:36<00:00,  1.50it/s, train_mse_loss_step=0.0457]Epoch 0: |          | 235/? [02:36<00:00,  1.50it/s, train_mse_loss_step=0.0496]Epoch 0: |          | 236/? [02:37<00:00,  1.50it/s, train_mse_loss_step=0.0496]Epoch 0: |          | 236/? [02:37<00:00,  1.50it/s, train_mse_loss_step=0.0446]Epoch 0: |          | 237/? [02:37<00:00,  1.50it/s, train_mse_loss_step=0.0446]Epoch 0: |          | 237/? [02:37<00:00,  1.50it/s, train_mse_loss_step=0.0489]Epoch 0: |          | 238/? [02:38<00:00,  1.50it/s, train_mse_loss_step=0.0489]Epoch 0: |          | 238/? [02:38<00:00,  1.50it/s, train_mse_loss_step=0.047] Epoch 0: |          | 239/? [02:38<00:00,  1.50it/s, train_mse_loss_step=0.047]Epoch 0: |          | 239/? [02:38<00:00,  1.50it/s, train_mse_loss_step=0.0503]Epoch 0: |          | 240/? [02:39<00:00,  1.50it/s, train_mse_loss_step=0.0503]Epoch 0: |          | 240/? [02:39<00:00,  1.50it/s, train_mse_loss_step=0.052] Epoch 0: |          | 241/? [02:40<00:00,  1.51it/s, train_mse_loss_step=0.052]Epoch 0: |          | 241/? [02:40<00:00,  1.51it/s, train_mse_loss_step=0.0479]Epoch 0: |          | 242/? [02:40<00:00,  1.51it/s, train_mse_loss_step=0.0479]Epoch 0: |          | 242/? [02:40<00:00,  1.51it/s, train_mse_loss_step=0.0476]Epoch 0: |          | 243/? [02:41<00:00,  1.51it/s, train_mse_loss_step=0.0476]Epoch 0: |          | 243/? [02:41<00:00,  1.51it/s, train_mse_loss_step=0.047] Epoch 0: |          | 244/? [02:41<00:00,  1.51it/s, train_mse_loss_step=0.047]Epoch 0: |          | 244/? [02:41<00:00,  1.51it/s, train_mse_loss_step=0.050]Epoch 0: |          | 245/? [02:42<00:00,  1.51it/s, train_mse_loss_step=0.050]Epoch 0: |          | 245/? [02:42<00:00,  1.51it/s, train_mse_loss_step=0.0453]Epoch 0: |          | 246/? [02:42<00:00,  1.51it/s, train_mse_loss_step=0.0453]Epoch 0: |          | 246/? [02:43<00:00,  1.51it/s, train_mse_loss_step=0.0488]Epoch 0: |          | 247/? [02:43<00:00,  1.51it/s, train_mse_loss_step=0.0488]Epoch 0: |          | 247/? [02:43<00:00,  1.51it/s, train_mse_loss_step=0.0445]Epoch 0: |          | 248/? [02:44<00:00,  1.51it/s, train_mse_loss_step=0.0445]Epoch 0: |          | 248/? [02:44<00:00,  1.51it/s, train_mse_loss_step=0.0458]Epoch 0: |          | 249/? [02:44<00:00,  1.51it/s, train_mse_loss_step=0.0458]Epoch 0: |          | 249/? [02:44<00:00,  1.51it/s, train_mse_loss_step=0.046] Epoch 0: |          | 250/? [02:45<00:00,  1.51it/s, train_mse_loss_step=0.046]Epoch 0: |          | 250/? [02:45<00:00,  1.51it/s, train_mse_loss_step=0.0421]Epoch 0: |          | 251/? [02:45<00:00,  1.51it/s, train_mse_loss_step=0.0421]Epoch 0: |          | 251/? [02:45<00:00,  1.51it/s, train_mse_loss_step=0.049] Epoch 0: |          | 252/? [02:46<00:00,  1.51it/s, train_mse_loss_step=0.049]Epoch 0: |          | 252/? [02:46<00:00,  1.51it/s, train_mse_loss_step=0.0474]Epoch 0: |          | 253/? [02:47<00:00,  1.51it/s, train_mse_loss_step=0.0474]Epoch 0: |          | 253/? [02:47<00:00,  1.51it/s, train_mse_loss_step=0.0437]Epoch 0: |          | 254/? [02:47<00:00,  1.52it/s, train_mse_loss_step=0.0437]Epoch 0: |          | 254/? [02:47<00:00,  1.52it/s, train_mse_loss_step=0.0494]Epoch 0: |          | 255/? [02:48<00:00,  1.52it/s, train_mse_loss_step=0.0494]Epoch 0: |          | 255/? [02:48<00:00,  1.52it/s, train_mse_loss_step=0.0437]Epoch 0: |          | 256/? [02:48<00:00,  1.52it/s, train_mse_loss_step=0.0437]Epoch 0: |          | 256/? [02:48<00:00,  1.52it/s, train_mse_loss_step=0.0438]Epoch 0: |          | 257/? [02:49<00:00,  1.52it/s, train_mse_loss_step=0.0438]Epoch 0: |          | 257/? [02:49<00:00,  1.52it/s, train_mse_loss_step=0.0502]Epoch 0: |          | 258/? [02:49<00:00,  1.52it/s, train_mse_loss_step=0.0502]Epoch 0: |          | 258/? [02:49<00:00,  1.52it/s, train_mse_loss_step=0.0442]Epoch 0: |          | 259/? [02:50<00:00,  1.52it/s, train_mse_loss_step=0.0442]Epoch 0: |          | 259/? [02:50<00:00,  1.52it/s, train_mse_loss_step=0.0482]Epoch 0: |          | 260/? [02:51<00:00,  1.52it/s, train_mse_loss_step=0.0482]Epoch 0: |          | 260/? [02:51<00:00,  1.52it/s, train_mse_loss_step=0.0455]Epoch 0: |          | 261/? [02:51<00:00,  1.52it/s, train_mse_loss_step=0.0455]Epoch 0: |          | 261/? [02:51<00:00,  1.52it/s, train_mse_loss_step=0.0421]Epoch 0: |          | 262/? [02:52<00:00,  1.52it/s, train_mse_loss_step=0.0421]Epoch 0: |          | 262/? [02:52<00:00,  1.52it/s, train_mse_loss_step=0.0473]Epoch 0: |          | 263/? [02:52<00:00,  1.52it/s, train_mse_loss_step=0.0473]Epoch 0: |          | 263/? [02:52<00:00,  1.52it/s, train_mse_loss_step=0.0497]Epoch 0: |          | 264/? [02:53<00:00,  1.52it/s, train_mse_loss_step=0.0497]Epoch 0: |          | 264/? [02:53<00:00,  1.52it/s, train_mse_loss_step=0.0432]Epoch 0: |          | 265/? [02:53<00:00,  1.52it/s, train_mse_loss_step=0.0432]Epoch 0: |          | 265/? [02:53<00:00,  1.52it/s, train_mse_loss_step=0.0483]Epoch 0: |          | 266/? [02:54<00:00,  1.52it/s, train_mse_loss_step=0.0483]Epoch 0: |          | 266/? [02:54<00:00,  1.52it/s, train_mse_loss_step=0.0427]Epoch 0: |          | 267/? [02:55<00:00,  1.52it/s, train_mse_loss_step=0.0427]Epoch 0: |          | 267/? [02:55<00:00,  1.52it/s, train_mse_loss_step=0.0439]Epoch 0: |          | 268/? [02:55<00:00,  1.53it/s, train_mse_loss_step=0.0439]Epoch 0: |          | 268/? [02:55<00:00,  1.53it/s, train_mse_loss_step=0.0448]Epoch 0: |          | 269/? [02:56<00:00,  1.53it/s, train_mse_loss_step=0.0448]Epoch 0: |          | 269/? [02:56<00:00,  1.53it/s, train_mse_loss_step=0.0482]Epoch 0: |          | 270/? [02:56<00:00,  1.53it/s, train_mse_loss_step=0.0482]Epoch 0: |          | 270/? [02:56<00:00,  1.53it/s, train_mse_loss_step=0.0443]Epoch 0: |          | 271/? [02:57<00:00,  1.53it/s, train_mse_loss_step=0.0443]Epoch 0: |          | 271/? [02:57<00:00,  1.53it/s, train_mse_loss_step=0.0462]Epoch 0: |          | 272/? [02:57<00:00,  1.53it/s, train_mse_loss_step=0.0462]Epoch 0: |          | 272/? [02:57<00:00,  1.53it/s, train_mse_loss_step=0.0423]Epoch 0: |          | 273/? [02:58<00:00,  1.53it/s, train_mse_loss_step=0.0423]Epoch 0: |          | 273/? [02:58<00:00,  1.53it/s, train_mse_loss_step=0.0469]Epoch 0: |          | 274/? [02:59<00:00,  1.53it/s, train_mse_loss_step=0.0469]Epoch 0: |          | 274/? [02:59<00:00,  1.53it/s, train_mse_loss_step=0.0457]Epoch 0: |          | 275/? [02:59<00:00,  1.53it/s, train_mse_loss_step=0.0457]Epoch 0: |          | 275/? [02:59<00:00,  1.53it/s, train_mse_loss_step=0.051] Epoch 0: |          | 276/? [03:00<00:00,  1.53it/s, train_mse_loss_step=0.051]Epoch 0: |          | 276/? [03:00<00:00,  1.53it/s, train_mse_loss_step=0.0457]Epoch 0: |          | 277/? [03:00<00:00,  1.53it/s, train_mse_loss_step=0.0457]Epoch 0: |          | 277/? [03:00<00:00,  1.53it/s, train_mse_loss_step=0.0432]Epoch 0: |          | 278/? [03:01<00:00,  1.53it/s, train_mse_loss_step=0.0432]Epoch 0: |          | 278/? [03:01<00:00,  1.53it/s, train_mse_loss_step=0.045] Epoch 0: |          | 279/? [03:01<00:00,  1.53it/s, train_mse_loss_step=0.045]Epoch 0: |          | 279/? [03:01<00:00,  1.53it/s, train_mse_loss_step=0.044]Epoch 0: |          | 280/? [03:02<00:00,  1.53it/s, train_mse_loss_step=0.044]Epoch 0: |          | 280/? [03:02<00:00,  1.53it/s, train_mse_loss_step=0.0441]Epoch 0: |          | 281/? [03:03<00:00,  1.53it/s, train_mse_loss_step=0.0441]Epoch 0: |          | 281/? [03:03<00:00,  1.53it/s, train_mse_loss_step=0.0397]Epoch 0: |          | 282/? [03:03<00:00,  1.54it/s, train_mse_loss_step=0.0397]Epoch 0: |          | 282/? [03:03<00:00,  1.54it/s, train_mse_loss_step=0.0446]Epoch 0: |          | 283/? [03:04<00:00,  1.54it/s, train_mse_loss_step=0.0446]Epoch 0: |          | 283/? [03:04<00:00,  1.54it/s, train_mse_loss_step=0.047] Epoch 0: |          | 284/? [03:04<00:00,  1.54it/s, train_mse_loss_step=0.047]Epoch 0: |          | 284/? [03:04<00:00,  1.54it/s, train_mse_loss_step=0.0405]Epoch 0: |          | 285/? [03:05<00:00,  1.54it/s, train_mse_loss_step=0.0405]Epoch 0: |          | 285/? [03:05<00:00,  1.54it/s, train_mse_loss_step=0.0416]Epoch 0: |          | 286/? [03:05<00:00,  1.54it/s, train_mse_loss_step=0.0416]Epoch 0: |          | 286/? [03:05<00:00,  1.54it/s, train_mse_loss_step=0.0445]Epoch 0: |          | 287/? [03:06<00:00,  1.54it/s, train_mse_loss_step=0.0445]Epoch 0: |          | 287/? [03:06<00:00,  1.54it/s, train_mse_loss_step=0.0463]Epoch 0: |          | 288/? [03:07<00:00,  1.54it/s, train_mse_loss_step=0.0463]Epoch 0: |          | 288/? [03:07<00:00,  1.54it/s, train_mse_loss_step=0.043] Epoch 0: |          | 289/? [03:07<00:00,  1.54it/s, train_mse_loss_step=0.043]Epoch 0: |          | 289/? [03:07<00:00,  1.54it/s, train_mse_loss_step=0.047]Epoch 0: |          | 290/? [03:08<00:00,  1.54it/s, train_mse_loss_step=0.047]Epoch 0: |          | 290/? [03:08<00:00,  1.54it/s, train_mse_loss_step=0.0474]Epoch 0: |          | 291/? [03:08<00:00,  1.54it/s, train_mse_loss_step=0.0474]Epoch 0: |          | 291/? [03:08<00:00,  1.54it/s, train_mse_loss_step=0.042] Epoch 0: |          | 292/? [03:09<00:00,  1.54it/s, train_mse_loss_step=0.042]Epoch 0: |          | 292/? [03:09<00:00,  1.54it/s, train_mse_loss_step=0.0422]Epoch 0: |          | 293/? [03:09<00:00,  1.54it/s, train_mse_loss_step=0.0422]Epoch 0: |          | 293/? [03:09<00:00,  1.54it/s, train_mse_loss_step=0.0444]Epoch 0: |          | 294/? [03:10<00:00,  1.54it/s, train_mse_loss_step=0.0444]Epoch 0: |          | 294/? [03:10<00:00,  1.54it/s, train_mse_loss_step=0.0435]Epoch 0: |          | 295/? [03:11<00:00,  1.54it/s, train_mse_loss_step=0.0435]Epoch 0: |          | 295/? [03:11<00:00,  1.54it/s, train_mse_loss_step=0.0401]Epoch 0: |          | 296/? [03:11<00:00,  1.54it/s, train_mse_loss_step=0.0401]Epoch 0: |          | 296/? [03:11<00:00,  1.54it/s, train_mse_loss_step=0.0435]Epoch 0: |          | 297/? [03:12<00:00,  1.55it/s, train_mse_loss_step=0.0435]Epoch 0: |          | 297/? [03:12<00:00,  1.54it/s, train_mse_loss_step=0.044] Epoch 0: |          | 298/? [03:12<00:00,  1.55it/s, train_mse_loss_step=0.044]Epoch 0: |          | 298/? [03:12<00:00,  1.55it/s, train_mse_loss_step=0.0416]Epoch 0: |          | 299/? [03:13<00:00,  1.54it/s, train_mse_loss_step=0.0416]Epoch 0: |          | 299/? [03:13<00:00,  1.54it/s, train_mse_loss_step=0.0433]Epoch 0: |          | 300/? [03:14<00:00,  1.54it/s, train_mse_loss_step=0.0433]Epoch 0: |          | 300/? [03:14<00:00,  1.54it/s, train_mse_loss_step=0.039] Epoch 0: |          | 300/? [03:14<00:00,  1.54it/s, train_mse_loss_step=0.039, train_mse_loss_epoch=0.113]`Trainer.fit` stopped: `max_steps=300` reached.
Epoch 0: |          | 300/? [03:14<00:00,  1.54it/s, train_mse_loss_step=0.039, train_mse_loss_epoch=0.113]
[2025-12-10 14:54:10,141][anemoi.utils.checkpoints][INFO] - Adding extra information to checkpoint /lus/h2resw01/hpcperm/nld4584/Anemoi_S2S_experiment/output_training/checkpoint/fdc4cf41-3ee2-43d0-946c-b63fbb17cb9c/inference-anemoi-by_time-epoch_000-step_000300.ckpt
[2025-12-10 14:54:10,142][anemoi.utils.checkpoints][INFO] - Saving metadata to inference-anemoi-by_time-epoch_000-step_000300/anemoi-metadata/ai-models.json
[2025-12-10 14:54:10,144][anemoi.utils.checkpoints][INFO] - Saving supporting array `latitudes` to inference-anemoi-by_time-epoch_000-step_000300/anemoi-metadata/latitudes.numpy (shape=(40320,), dtype=float64)
[2025-12-10 14:54:10,145][anemoi.utils.checkpoints][INFO] - Saving supporting array `longitudes` to inference-anemoi-by_time-epoch_000-step_000300/anemoi-metadata/longitudes.numpy (shape=(40320,), dtype=float64)
[2025-12-10 14:54:22,681][anemoi.utils.checkpoints][INFO] - Adding extra information to checkpoint /lus/h2resw01/hpcperm/nld4584/Anemoi_S2S_experiment/output_training/checkpoint/fdc4cf41-3ee2-43d0-946c-b63fbb17cb9c/anemoi-by_time-epoch_000-step_000300.ckpt
[2025-12-10 14:54:22,682][anemoi.utils.checkpoints][INFO] - Saving metadata to archive/anemoi-metadata/ai-models.json
[2025-12-10 14:54:22,683][anemoi.utils.checkpoints][INFO] - Saving supporting array `latitudes` to archive/anemoi-metadata/latitudes.numpy (shape=(40320,), dtype=float64)
[2025-12-10 14:54:22,684][anemoi.utils.checkpoints][INFO] - Saving supporting array `longitudes` to archive/anemoi-metadata/longitudes.numpy (shape=(40320,), dtype=float64)
[2025-12-10 14:54:24,625][anemoi.utils.checkpoints][INFO] - Adding extra information to checkpoint /lus/h2resw01/hpcperm/nld4584/Anemoi_S2S_experiment/output_training/checkpoint/fdc4cf41-3ee2-43d0-946c-b63fbb17cb9c/inference-last.ckpt
[2025-12-10 14:54:24,625][anemoi.utils.checkpoints][INFO] - Saving metadata to inference-last/anemoi-metadata/ai-models.json
[2025-12-10 14:54:24,627][anemoi.utils.checkpoints][INFO] - Saving supporting array `latitudes` to inference-last/anemoi-metadata/latitudes.numpy (shape=(40320,), dtype=float64)
[2025-12-10 14:54:24,628][anemoi.utils.checkpoints][INFO] - Saving supporting array `longitudes` to inference-last/anemoi-metadata/longitudes.numpy (shape=(40320,), dtype=float64)
[2025-12-10 14:54:35,388][anemoi.utils.checkpoints][INFO] - Adding extra information to checkpoint /lus/h2resw01/hpcperm/nld4584/Anemoi_S2S_experiment/output_training/checkpoint/fdc4cf41-3ee2-43d0-946c-b63fbb17cb9c/last.ckpt
[2025-12-10 14:54:35,389][anemoi.utils.checkpoints][INFO] - Saving metadata to archive/anemoi-metadata/ai-models.json
[2025-12-10 14:54:35,390][anemoi.utils.checkpoints][INFO] - Saving supporting array `latitudes` to archive/anemoi-metadata/latitudes.numpy (shape=(40320,), dtype=float64)
[2025-12-10 14:54:35,391][anemoi.utils.checkpoints][INFO] - Saving supporting array `longitudes` to archive/anemoi-metadata/longitudes.numpy (shape=(40320,), dtype=float64)
[2025-12-10 14:54:37,252][anemoi.utils.checkpoints][INFO] - Adding extra information to checkpoint /lus/h2resw01/hpcperm/nld4584/Anemoi_S2S_experiment/output_training/checkpoint/fdc4cf41-3ee2-43d0-946c-b63fbb17cb9c/inference-anemoi-by_epoch-epoch_000-step_000300.ckpt
[2025-12-10 14:54:37,253][anemoi.utils.checkpoints][INFO] - Saving metadata to inference-anemoi-by_epoch-epoch_000-step_000300/anemoi-metadata/ai-models.json
[2025-12-10 14:54:37,255][anemoi.utils.checkpoints][INFO] - Saving supporting array `latitudes` to inference-anemoi-by_epoch-epoch_000-step_000300/anemoi-metadata/latitudes.numpy (shape=(40320,), dtype=float64)
[2025-12-10 14:54:37,255][anemoi.utils.checkpoints][INFO] - Saving supporting array `longitudes` to inference-anemoi-by_epoch-epoch_000-step_000300/anemoi-metadata/longitudes.numpy (shape=(40320,), dtype=float64)
[2025-12-10 14:54:47,862][anemoi.utils.checkpoints][INFO] - Adding extra information to checkpoint /lus/h2resw01/hpcperm/nld4584/Anemoi_S2S_experiment/output_training/checkpoint/fdc4cf41-3ee2-43d0-946c-b63fbb17cb9c/anemoi-by_epoch-epoch_000-step_000300.ckpt
[2025-12-10 14:54:47,862][anemoi.utils.checkpoints][INFO] - Saving metadata to archive/anemoi-metadata/ai-models.json
[2025-12-10 14:54:47,864][anemoi.utils.checkpoints][INFO] - Saving supporting array `latitudes` to archive/anemoi-metadata/latitudes.numpy (shape=(40320,), dtype=float64)
[2025-12-10 14:54:47,865][anemoi.utils.checkpoints][INFO] - Saving supporting array `longitudes` to archive/anemoi-metadata/longitudes.numpy (shape=(40320,), dtype=float64)
[2025-12-10 14:54:49,054][anemoi.utils.checkpoints][INFO] - Adding extra information to checkpoint /lus/h2resw01/hpcperm/nld4584/Anemoi_S2S_experiment/output_training/checkpoint/fdc4cf41-3ee2-43d0-946c-b63fbb17cb9c/inference-last.ckpt
[2025-12-10 14:54:49,054][anemoi.utils.checkpoints][INFO] - Saving metadata to inference-last/anemoi-metadata/ai-models.json
[2025-12-10 14:54:49,056][anemoi.utils.checkpoints][INFO] - Saving supporting array `latitudes` to inference-last/anemoi-metadata/latitudes.numpy (shape=(40320,), dtype=float64)
[2025-12-10 14:54:49,057][anemoi.utils.checkpoints][INFO] - Saving supporting array `longitudes` to inference-last/anemoi-metadata/longitudes.numpy (shape=(40320,), dtype=float64)
[2025-12-10 14:54:59,600][anemoi.utils.checkpoints][INFO] - Adding extra information to checkpoint /lus/h2resw01/hpcperm/nld4584/Anemoi_S2S_experiment/output_training/checkpoint/fdc4cf41-3ee2-43d0-946c-b63fbb17cb9c/last.ckpt
[2025-12-10 14:54:59,600][anemoi.utils.checkpoints][INFO] - Saving metadata to archive/anemoi-metadata/ai-models.json
[2025-12-10 14:54:59,602][anemoi.utils.checkpoints][INFO] - Saving supporting array `latitudes` to archive/anemoi-metadata/latitudes.numpy (shape=(40320,), dtype=float64)
[2025-12-10 14:54:59,603][anemoi.utils.checkpoints][INFO] - Saving supporting array `longitudes` to archive/anemoi-metadata/longitudes.numpy (shape=(40320,), dtype=float64)
[2025-12-10 14:55:01,345][anemoi.training.diagnostics.callbacks.plot][INFO] - Teardown of the Plot Callback ...
[2025-12-10 14:55:01,345][anemoi.training.diagnostics.callbacks.plot][INFO] - waiting and shutting down the executor ...
[2025-12-10 14:55:01,345][anemoi.training.diagnostics.callbacks.plot][INFO] - Teardown of the Plot Callback ...
[2025-12-10 14:55:01,345][anemoi.training.diagnostics.callbacks.plot][INFO] - waiting and shutting down the executor ...
[2025-12-10 14:55:01,345][anemoi.training.diagnostics.callbacks.plot][INFO] - Teardown of the Plot Callback ...
[2025-12-10 14:55:01,345][anemoi.training.diagnostics.callbacks.plot][INFO] - waiting and shutting down the executor ...
[2025-12-10 14:55:01,346][anemoi.training.diagnostics.callbacks.plot][INFO] - Teardown of the Plot Callback ...
[2025-12-10 14:55:01,346][anemoi.training.diagnostics.callbacks.plot][INFO] - waiting and shutting down the executor ...
[2025-12-10 14:55:01,346][anemoi.training.diagnostics.callbacks.plot][INFO] - Teardown of the Plot Callback ...
[2025-12-10 14:55:01,346][anemoi.training.diagnostics.callbacks.plot][INFO] - waiting and shutting down the executor ...
[ECMWF-INFO -ecepilog] ----------------------------------------------------------------------------------------------------
[ECMWF-INFO -ecepilog] This is the ECMWF job Epilogue
[ECMWF-INFO -ecepilog] +++ Please report issues using the Support portal +++
[ECMWF-INFO -ecepilog] +++ https://support.ecmwf.int                     +++
[ECMWF-INFO -ecepilog] ----------------------------------------------------------------------------------------------------
[ECMWF-INFO -ecepilog] Run at 2025-12-10T14:55:20 on ac
[ECMWF-INFO -ecepilog] JobName                   : test_job
[ECMWF-INFO -ecepilog] JobID                     : 36910671
[ECMWF-INFO -ecepilog] Submit                    : 2025-12-10T14:32:05
[ECMWF-INFO -ecepilog] Start                     : 2025-12-10T14:32:08
[ECMWF-INFO -ecepilog] End                       : 2025-12-10T14:55:20
[ECMWF-INFO -ecepilog] QueuedTime                : 3.0
[ECMWF-INFO -ecepilog] ElapsedRaw                : 1392
[ECMWF-INFO -ecepilog] ExitCode                  : 0:0
[ECMWF-INFO -ecepilog] DerivedExitCode           : 0:0
[ECMWF-INFO -ecepilog] State                     : COMPLETED
[ECMWF-INFO -ecepilog] Account                   : nlcko
[ECMWF-INFO -ecepilog] QOS                       : ng
[ECMWF-INFO -ecepilog] User                      : nld4584
[ECMWF-INFO -ecepilog] StdOut                    : /ec/res4/hpcperm/nld4584/Anemoi_S2S_experiment/./slurm_scripts/output_slurm/hello-36910671.out
[ECMWF-INFO -ecepilog] StdErr                    : /ec/res4/hpcperm/nld4584/Anemoi_S2S_experiment/./slurm_scripts/output_slurm/hello-36910671.out
[ECMWF-INFO -ecepilog] NNodes                    : 1
[ECMWF-INFO -ecepilog] NCPUS                     : 2
[ECMWF-INFO -ecepilog] SBU                       : 6.598
[ECMWF-INFO -ecepilog] jobtag                    : nld4584-test_job-1x2-/ec/res4/hpcperm/nld4584/Anemoi_S2S_experiment/./slurm_scripts/output_slurm/hello-_JOBID_.out
[ECMWF-INFO -ecepilog] ----------------------------------------------------------------------------------------------------

