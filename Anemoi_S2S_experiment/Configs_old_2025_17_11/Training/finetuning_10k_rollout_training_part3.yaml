defaults:
- data: zarr
- dataloader: native_grid
- datamodule: single
- diagnostics: evaluation
- hardware: slurm
- graph: encoder_decoder_only
- model: transformer
- training: default
- _self_

config_validation: False

### This file is for local experimentation.
##  When you commit your changes, assign the new features and keywords
##  to the correct defaults.
# For example to change from default GPU count:
# hardware:
#   num_gpus_per_node: 1


data:
  normalizer:
    default: "mean-std"

diagnostics:
  log:
    mlflow:
      enabled: True
      offline: True
      authentication: True
      experiment_name: 'Louca_Vuilleumier_KNMI_S2S_experiment'
      project_name: 'Anemoi'
      log_model: False
      tracking_uri: 'https://mlflow.ecmwf.int'
      http_max_retries: 2
      run_name: 'finetuning_10k_rollout_training_part3'
    wandb:
      enabled: False
      entity: None

hardware:
  accelerator: auto
  num_gpus_per_model: 1
  paths:
    data: "/home/mlx/ai-ml/datasets/"
    output: "./output_training/"
    graph: "./graphs/"
    warm_start: "/ec/res4/hpcperm/nld4584/Anemoi_S2S_experiment/output_training/checkpoint/5892ee4f3c1048da9e67e53383099cab"

  files:
    dataset: aifs-ea-an-oper-0001-mars-o96-1979-2023-6h-v8.zarr
    graph: graph.pkl
    warm_start: last.ckpt
  

model:
  keep_batch_sharded: True
  num_channels: 1024
dataloader:
  limit_batches:
    training: 100
    validation: 100

training:

  transfer_learning: False
  rollout:
    start: 22
    # increase rollout every n epochs
    epoch_increment: 3
    # maximum rollout to use
    max: 28
    
  max_steps: 320001  # Checkpoint is at 310001 steps (Mariana checkpoint) + 10000 additional steps
  max_epochs: null
  lr:
    warmup: 0
    rate: 6.25e-5 #or 1.5625e-5 if divided by 4 (scaled for 4 GPUs)
    iterations: ${training.max_steps}
    min: 3e-7 #Not scaled by #GPU

